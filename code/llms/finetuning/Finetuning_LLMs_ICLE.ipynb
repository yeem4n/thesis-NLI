{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction-tuning LLMs for Native Language Identification on ICLE"
      ],
      "metadata": {
        "id": "fVOfZxsbi8o8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains the code for instruction-tuning open-source large language models on the task of Native Language Identification. The code is heavily inspired by https://github.com/unslothai/unsloth and their example notebooks on fine-tuning LLMs. We will be using the Unsloth library to perform 4-bit QLoRA fine-tuning on the ICLE-NLI dataset under stratified 5-fold cross validation. We recommend running this notebook in Google Colaboratory to speed up the fine-tuning process."
      ],
      "metadata": {
        "id": "Lg70c3_Ak9Bi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip install imbalanced-learn\n",
        "!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive if using Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sXshB_R3pgSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f11ef1-81c9-4b99-b63f-6f6286084a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset, Dataset\n",
        "from pydantic import BaseModel, ValidationError, Field\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from typing import Literal\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score, accuracy_score\n",
        "from collections import defaultdict\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import Dataset\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = torch.bfloat16\n",
        "load_in_4bit = True # we use 4bit quantization"
      ],
      "metadata": {
        "id": "qDNYHwfBCc4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7a9803-8e1d-4720-8709-9c8c64172f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "96e08f0b-824a-4f51-fe88-dc23104766eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.8: Fast Mistral patching. Transformers = 4.43.3.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# 4bit pre quantized models supported by Unsloth\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\"\n",
        "]\n",
        "\n",
        "run_name = 'finetuned_mistral'\n",
        "model_name = \"unsloth/mistral-7b-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "# model = FastLanguageModel.get_peft_model(\n",
        "#       model,\n",
        "#       r = 16,\n",
        "#       target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "#       lora_alpha = 16,\n",
        "#       lora_dropout = 0,\n",
        "#       bias = \"none\",\n",
        "#       use_gradient_checkpointing = \"unsloth\",\n",
        "#       random_state = 777,\n",
        "#       use_rslora = False,\n",
        "#       loftq_config = None,\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt):\n",
        "  \"\"\"\n",
        "  Generate text for LLM based on input prompt\n",
        "  :param prompt: input prompt\n",
        "  :param max_length:\n",
        "  :type prompt: str\n",
        "  :type max_length: int\n",
        "  \"\"\"\n",
        "    # Tokenize the prompt\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "  outputs = model.generate(inputs,\n",
        "                           max_new_tokens=10,\n",
        "                           pad_token_id=tokenizer.eos_token_id,\n",
        "                           )\n",
        "    # Decode the response\n",
        "  response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  return response\n",
        "\n",
        "def clean_output(output, eos_token, output_only=False):\n",
        "  \"\"\"\n",
        "  This function specifically cleans up the output given by Llama 2 chat,\n",
        "  to remove the prompt, make sure it is in the correct format, and remove any empty lines.\n",
        "  :param output: model generated output\n",
        "  :param labels: all possible classes in TOEFL/ICLE\n",
        "  :param eos_token: end-of-sequence token to split model output on\n",
        "  :param output_only, default False: if True, extract only the newly generated output by model and remove the prompt. mostly used for debugging\n",
        "  :type output: str\n",
        "  :type labels: list\n",
        "  :type eos_token: str\n",
        "  \"\"\"\n",
        "  pure_output = output.split(eos_token)\n",
        "  pure_output = pure_output[-1]\n",
        "  pure_output = pure_output.strip()\n",
        "  final_output = pure_output\n",
        "  if output_only==False: # whether to extract only json-formatted string in the output or not\n",
        "    predicted_classes=0\n",
        "    language_class_dict = {'arabic': 'ARA',\n",
        "                            'bulgarian': 'BUL',\n",
        "                             'chinese': 'CHI',\n",
        "                             'czech': 'CZE',\n",
        "                             \"french\": \"FRE\",\n",
        "                             \"german\": \"GER\",\n",
        "                             \"hindi\": \"HIN\",\n",
        "                             \"italian\": \"ITA\",\n",
        "                             \"japanese\": \"JPN\",\n",
        "                             \"korean\": 'KOR',\n",
        "                             \"spanish\": \"SPA\",\n",
        "                             \"telugu\": \"TEL\",\n",
        "                             \"turkish\": \"TUR\",\n",
        "                             \"russian\": \"RUS\",\n",
        "                             \"english\": \"ENG\",\n",
        "                            'sp': 'SPA'\n",
        "                             }\n",
        "    if '}' in final_output:\n",
        "      x = output.split(\"}\")\n",
        "      for piece in x:\n",
        "        if 'native_lang' in piece:\n",
        "          x = piece.split(\":\")\n",
        "          label = x[-1]\n",
        "          label = label.strip()\n",
        "          label = label.replace('\"', '')\n",
        "          label = label.replace('\\n', '')\n",
        "          final_output = '{\"native_lang\":\"' + label + '\"}'\n",
        "    if 'Class:' in final_output:\n",
        "      x = output.split(\"Class:\")\n",
        "      label = x[-1]\n",
        "      label = label.strip()\n",
        "      label = label.replace('\"', '')\n",
        "      label = label.replace('\\n', '')\n",
        "      label = label.replace('.', '')\n",
        "      final_output = label\n",
        "    for lang, label in language_class_dict.items():\n",
        "      if lang in final_output.lower() or label in final_output:\n",
        "        final_output = '{\"native_lang\":\"' + label + '\"}'\n",
        "\n",
        "  return final_output\n",
        "\n",
        "prompt_retry_eng = '''\n",
        "  You previously mistakenly predicted this text as \"ENG\" (English). The class is NOT English.\n",
        "  Please classify the native language of the author of the text again.\n",
        "  '''\n",
        "\n",
        "def classify(texts, goldlabels, filter_token, closedopen_setting ='closed'):\n",
        "  '''\n",
        "  :param texts: list of texts\n",
        "  :param goldlabels: list of gold labels\n",
        "  :param dataset: TOEFL or ICLE\n",
        "  :param filter_token: token to get cleaned output\n",
        "  :type texts: list\n",
        "  :type goldlabels: list\n",
        "  :type dataset: str\n",
        "  :returns predictions: a list of model predictions\n",
        "  '''\n",
        "  predictions = []\n",
        "  # if closedopen_setting != 'open':\n",
        "  sys_prompt = prompt_ICLE\n",
        "  prompt_retry = prompt_retry_ICLE\n",
        "  all_labels = all_labels_ICLE\n",
        "  NLI_prediction = NLI_prediction_ICLE\n",
        "  main_task_prompt = main_task_prompt_ICLE\n",
        "  count = 1\n",
        "  for text, gold in zip(texts, goldlabels):\n",
        "    promptcounter = 0\n",
        "    while True:\n",
        "      try:\n",
        "        fullprompt = \"Instruction: \" + sys_prompt + '\\n\\n' + main_task_prompt + '\\n\\nInput: '+ text + \"\\nResponse:\" # use same prompt template as used in fine-tuning\n",
        "        output = generate_text(fullprompt) # generate output per text\n",
        "        output_only = clean_output(output, filter_token, output_only=True)\n",
        "        # print(output_only)\n",
        "        final_output = clean_output(output, filter_token)\n",
        "        validated_response = NLI_prediction.model_validate_json(final_output) # use class to validate json string\n",
        "        response_dict = validated_response.model_dump() # dump validated response into dict\n",
        "        predicted_native_lang = response_dict['native_lang'] # get the predicted native language\n",
        "        if predicted_native_lang == \"ENG\": # reiterate prompt if model predicts english\n",
        "          fullprompt = \"Instruction: \" + sys_prompt + '\\n\\n' + main_task_prompt + '\\n' + prompt_retry_eng + '\\n\\nInput: '+ text + \"\\nResponse:\"\n",
        "          promptcounter+=1\n",
        "          if promptcounter > 4: # try 5 times to reprompt, if still unable to extract predicted label, append other\n",
        "            response_dict = {'native_lang': 'other'}\n",
        "            predictions.append('other')\n",
        "            break\n",
        "        else:\n",
        "          predictions.append(predicted_native_lang) # append it to list of predictions\n",
        "          break\n",
        "      # print(final_output)\n",
        "      except ValidationError as e: # if there is a validation error, make model retry\n",
        "        fullprompt = \"Instruction: \" + sys_prompt + '\\n\\n' + main_task_prompt + '\\n' + prompt_retry + '\\n\\nInput: '+ text + \"\\nResponse:\"\n",
        "        promptcounter +=1\n",
        "        if promptcounter > 4: # try 5 times to reprompt, if still unable to extract predicted label, append other\n",
        "          response_dict = {'native_lang': 'other'}\n",
        "          predictions.append('other')\n",
        "          break\n",
        "    print(count, response_dict)\n",
        "    print('Accuracy:', \"{:.2f}\".format(accuracy_score(goldlabels[0:count], predictions)*100))\n",
        "    count +=1\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "dDpVjmVssOpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLI_prediction_ICLE(BaseModel):\n",
        "  native_lang: Literal['BUL', 'CHI', 'CZE', 'FRE', 'JPN', 'RUS', 'SPA']\n",
        "\n",
        "all_labels_ICLE = ['BUL', 'CHI', 'CZE', 'FRE', 'JPN', 'RUS', 'SPA']\n",
        "\n",
        "prompt_ICLE = '''\n",
        "  You are a forensic linguistics expert that reads English texts written by non-native authors to classify the native language of the author as one of:\n",
        "  \"BUL\": Bulgarian\n",
        "  \"CHI\": Chinese\n",
        "  \"CZE\": Czech\n",
        "  \"FRE\": French\n",
        "  \"JPN\": Japanese\n",
        "  \"RUS\": Russian\n",
        "  \"SPA\": Spanish\n",
        "  Use clues such as spelling errors, word choice, syntactic patterns, and grammatical errors to decide on the native language of the author.\\n\\n\n",
        "\n",
        "  DO NOT USE ANY OTHER CLASS.\n",
        "  IMPORTANT: Do not classify any input as \"ENG\" (English). English is an invalid choice.\n",
        "\n",
        "  Valid output formats:\n",
        "  Class: \"BUL\"\n",
        "  Class: \"CHI\"\n",
        "  Class: \"CZE\"\n",
        "  Class: \"SPA\"\n",
        "  '''\n",
        "\n",
        "main_task_prompt_ICLE = '''Classify the text above as one of BUL, CHI, CZE, FRE, JPN, RUS, or SPA. Do not output any other class - do NOT choose \"ENG\" (English). What is the closest native language of the author of this English text from the given list?'''\n",
        "\n",
        "prompt_retry_ICLE = '''\n",
        "  Your classification is not in the list of possible languages.\n",
        "  Please try again and choose only one of the following classes:\n",
        "  BUL, CHI, CZE, FRE, JPN, RUS, or SPA\n",
        "  '''"
      ],
      "metadata": {
        "id": "6wk-Y7EntUhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "icle = \"/content/drive/MyDrive/thesis_NLI/ICLE-NLI-results-final.csv\"\n",
        "\n",
        "alpaca_prompt = '''\n",
        "### Instruction:\n",
        "You are a forensic linguistics expert that reads English texts written by non-native authors to classify the native language of the author as one of:\n",
        "\"BUL\": Bulgarian\n",
        "\"CHI\": Chinese\n",
        "\"CZE\": Czech\n",
        "\"FRE\": French\n",
        "\"JPN\": Japanese\n",
        "\"RUS\": Russian\n",
        "\"SPA\": Spanish\n",
        "Use clues such as spelling errors, word choice, syntactic patterns, and grammatical errors to decide on the native language of the author.\\n\\n\n",
        "\n",
        "DO NOT USE ANY OTHER CLASS.\n",
        "IMPORTANT: Do not classify any input as \"ENG\" (English). English is an invalid choice.\n",
        "\n",
        "Valid output formats:\n",
        "Class: \"BUL\"\n",
        "Class: \"CHI\"\n",
        "Class: \"CZE\"\n",
        "Class: \"SPA\"\n",
        "\n",
        "Classify the text below as one of BUL, CHI, CZE, FRE, JPN, RUS, or SPA. Do not output any other class - do NOT choose \"ENG\" (English). What is the closest native language of the author of this English text from the given list?\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}'''\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "df_icle = pd.read_csv(icle)\n",
        "x = df_icle['text'].tolist()\n",
        "y = df_icle['language'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs       = examples[\"text\"]\n",
        "    outputs      = examples[\"language\"]\n",
        "    texts = []\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ],
      "metadata": {
        "id": "GmmKEcGf9_HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ind, rs in zip(range(0,1), [100]): # 3 runs with 3 random seeds if necessary\n",
        "  count=0\n",
        "  accuracies=[]\n",
        "  all_predictions = []\n",
        "  all_test_index = []\n",
        "  for tr_index, (train_index, test_index) in enumerate(skf.split(x, y)):\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"unsloth/mistral-7b-bnb-4bit\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "        )\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r = 16,\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "        lora_alpha = 16,\n",
        "        lora_dropout = 0,\n",
        "        bias = \"none\",\n",
        "        use_gradient_checkpointing = \"unsloth\",\n",
        "        random_state = rs,\n",
        "        use_rslora = False,\n",
        "        loftq_config = None,\n",
        "      )\n",
        "    x_train_fold, x_test_fold = np.array(x)[train_index], np.array(x)[test_index]\n",
        "    y_train_fold, y_test_fold = np.array(y)[train_index], np.array(y)[test_index]\n",
        "    train_dict = {'text': x_train_fold, 'language': y_train_fold}\n",
        "    # perform random undersampling\n",
        "    rus = RandomUnderSampler(sampling_strategy={'BUL':33, 'CHI':33, 'CZE':33, 'FRE':33, 'JPN':33, 'RUS':33, 'SPA':33}, random_state=0)\n",
        "    df = pd.DataFrame(train_dict)\n",
        "    X = df.drop('language',axis=1)\n",
        "    Y = df['language'].tolist()\n",
        "    X_resampled, y_resampled = rus.fit_resample(X, Y)\n",
        "    sample_df = pd.DataFrame({'text': X_resampled['text'].tolist(), 'language': y_resampled})\n",
        "\n",
        "    dataset = Dataset.from_pandas(sample_df)\n",
        "    print(Counter(y_resampled))\n",
        "\n",
        "    #uncomment the following code for the full training set\n",
        "    # dataset = Dataset.from_dict(train_dict)\n",
        "    dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "    trainer = SFTTrainer(\n",
        "      model = model,\n",
        "      tokenizer = tokenizer,\n",
        "      train_dataset = dataset,\n",
        "      dataset_text_field = \"text\",\n",
        "      max_seq_length = max_seq_length,\n",
        "      dataset_num_proc = 2,\n",
        "      packing = False, # Can make training 5x faster for short sequences.\n",
        "      args = TrainingArguments(\n",
        "          per_device_train_batch_size = 4, # The batch size per GPU/TPU core/CPU for training.\n",
        "          gradient_accumulation_steps = 4,\n",
        "          warmup_steps = 5,\n",
        "          #max_steps = 0,\n",
        "          num_train_epochs=3,\n",
        "          learning_rate = 1e-4, # originally 2e-4\n",
        "          fp16 = not torch.cuda.is_bf16_supported(),\n",
        "          bf16 = torch.cuda.is_bf16_supported(),\n",
        "          logging_steps = 1,\n",
        "          optim = \"adamw_8bit\",\n",
        "          weight_decay = 0.01,\n",
        "          lr_scheduler_type = \"linear\",\n",
        "          seed = rs,\n",
        "          output_dir = \"outputs\",\n",
        "      ),\n",
        "      )\n",
        "    trainer_stats = trainer.train()\n",
        "    model.save_pretrained(f\"/content/drive/MyDrive/thesis_NLI/ICLE/finetuned_mistral_undersampled_{count}\") # Local saving\n",
        "    tokenizer.save_pretrained(f\"/content/drive/MyDrive/thesis_NLI/ICLE/finetuned_mistral_undersampled_{count}\")\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    eos_token = 'Response:'\n",
        "    results_ICLE = \"/content/drive/MyDrive/thesis_NLI/ICLE-NLI-results-final.csv\"\n",
        "    predictions = classify(x_test_fold, y_test_fold, eos_token)\n",
        "    accuracy = accuracy_score(y_test_fold, predictions)\n",
        "    accuracy2 = \"{:.2f}\".format(accuracy*100)\n",
        "\n",
        "    print(f'-------------Run: {count}')\n",
        "    print(f'-------------Accuracy: {accuracy2}')\n",
        "    accuracy=float(accuracy)\n",
        "    accuracies.append(accuracy)\n",
        "    all_test_index.extend(test_index.tolist())\n",
        "    all_predictions.extend(predictions)\n",
        "    count+=1\n",
        "\n",
        "  avg_acc=sum(accuracies)/5\n",
        "  print(f\"Average: {avg_acc*100}\")\n",
        "  print(f\"standard dev: {np.std(accuracies)}\")\n",
        "  print(accuracies)\n",
        "\n",
        "#save results to CSV file\n",
        "  results_dict = {}\n",
        "  from collections import OrderedDict\n",
        "  for index, pred in zip(all_test_index, all_predictions):\n",
        "    results_dict[index] = pred\n",
        "  print(results_dict)\n",
        "  sorted_dict = OrderedDict(sorted(results_dict.items()))\n",
        "  sorted_predictions = list(sorted_dict.values())\n",
        "  icle = \"/content/drive/MyDrive/thesis_NLI/ICLE-NLI-results-final.csv\"\n",
        "  df = pd.read_csv(icle)\n",
        "  num_columns = len(df.columns)\n",
        "  # df.drop(column_name, axis=1)\n",
        "  df.insert(num_columns, run_name, sorted_predictions)\n",
        "  df.to_csv(icle, index=False)\n",
        "  cm = confusion_matrix(y, sorted_predictions, labels=all_labels_ICLE)\n",
        "  cm_display = ConfusionMatrixDisplay(cm, display_labels=all_labels_ICLE).plot()\n",
        "  cm_display.figure_.savefig(f'/content/drive/MyDrive/thesis_NLI/ICLE_results/{run_name}.png')"
      ],
      "metadata": {
        "id": "6UKUwt2EXXIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4R9T6KhoPCqb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}