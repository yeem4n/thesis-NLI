{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZADQYRFvd_9n"
      },
      "source": [
        "# Instruction-tuning LLMs for Native Language Identification on VESPA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgpyMkg6kmVi"
      },
      "source": [
        "This notebook contains the code for instruction-tuning open-source large language models on the task of Native Language Identification. The code is heavily inspired by https://github.com/unslothai/unsloth and their example notebooks on fine-tuning LLMs. We will be using the Unsloth library to perform 4-bit QLoRA fine-tuning on a VESPA training set. We then evaluate the fine-tuned LLM on a small VESPA test set. We recommend running this notebook in Google Colaboratory to speed up the fine-tuning process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install tiktoken\n",
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDpVjmVssOpI",
        "outputId": "614a2c89-a917-4a83-8b42-dfc0d2cae965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive if using Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "3daf1888-8ef3-47c2-d219-6cda32b44857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset, Dataset\n",
        "from pydantic import BaseModel, ValidationError, Field\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from typing import Literal\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score, accuracy_score\n",
        "from collections import defaultdict\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import tiktoken\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "max_seq_length = 8000\n",
        "dtype = torch.bfloat16\n",
        "load_in_4bit = True # we use 4bit quantization\n",
        "\n",
        "# 4bit pre quantized models supported by Unsloth\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "6a5c2fdf-f028-4fb1-b698-326ab9e81dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.8: Fast Mistral patching. Transformers = 4.43.3.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "model_name = \"unsloth/mistral-7b-bnb-4bit\"\n",
        "run_name = 'finetuned_mistral_undersampled'\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        "  )\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBTmkWYJbuK8"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_string(string: str):\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "  length = inputs[0].size(dim=0)\n",
        "  return int(length)\n",
        "\n",
        "def truncate_text(text, max_seq_length):\n",
        "  \"\"\"\n",
        "  give truncated_text\n",
        "  :param prompt: input prompt\n",
        "  :param max_length:\n",
        "  :type prompt: str\n",
        "  :type max_length: int\n",
        "  \"\"\"\n",
        "    # Tokenize the prompt\n",
        "  inputs = tokenizer(text, max_length = max_seq_length, truncation=True, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "  # print(inputs[0].size()) this is how to get the size of the tensor aka the number of tokens\n",
        "    # Decode the response\n",
        "  response = tokenizer.decode(inputs[0], skip_special_tokens=True)\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "# Data Prep\n",
        "We use the VESPA training set to perform instruction-tuning with the prompts used in the closed-set experiments, in which we provide the set of possible L1s in the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "885af4dda23246428187bf4cf13f32c5",
            "a65919859e4d4b8c8ba4c16d1f3aa328",
            "03bf1fcbadb5468fafe2c0b596aeb590",
            "5950326afc4442eda287f17ad81a63fb",
            "ffdc9f17bf9e4cb081f8b1d1a43d661d",
            "d869449b629a4ca880bf452a130ed586",
            "703736c8fe5c4a01938ca58bb5f61535",
            "df1becd2ed9b426fa4b2d2c05d411861",
            "ffd55ed65361436c93d471f53564ff9c",
            "994f0b6504134bff8cd091b3eeb4e376",
            "677d2b2799d64008bdb05b4a7767564a"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "18320d60-06d4-46e9-f626-7c9c51937281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "165\n",
            "7900\n",
            "Counter({'DUT': 33, 'FRE': 33, 'NOR': 33, 'SPA': 33, 'SWE': 33})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/165 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "885af4dda23246428187bf4cf13f32c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['\\n### Instruction:\\nYou are a forensic linguistics expert that reads English texts written by non-native authors to classify the native language of the author as one of:\\n\"DUT\": Dutch\\n\"FRE\": French\\n\"NOR\": Norwegian\\n\"SPA\": Spanish\\n\"SWE\": Swedish\\nUse clues such as spelling errors, word choice, syntactic patterns, and grammatical errors to decide on the native language of the author.\\n\\n\\n\\nDO NOT USE ANY OTHER CLASS.\\nIMPORTANT: Do not classify any input as \"ENG\" (English). English is an invalid choice.\\n\\nValid output formats:\\nClass: \"DUT\"\\nClass: \"SWE\"\\nClass: \"NOR\"\\nClass: \"SPA\"\\n\\nClassify the text above as one of DUT, FRE, NOR, SPA, or SWE. Do not output any other class - do NOT choose \"ENG\" (English). What is the closest native language of the author of this English text from the given list?\\n\\n### Input:\\nUndergraduate writing has recently become an essential topic in the field of English for Academic Purposes. An important aspect of undergraduate writing is the stance that the academics to be, take in their essays . The stance decides the way the writer engages him/herself and others in the text. This stance can be towards the subject matter , status of knowledge or the way in which the writer relates to the putative reader . Hyland argues that these strategies are employed to fulfil two functions. The first is the \" Acknowledgement of the need to adequately meet readers\\' expectations of inclusion and disciplinary solidarity. Here we find readers addressed as participants in an argument with reader pronouns and interjections. \" And the second is \" To rhetorically position the audience. Here the writer pulls readers into the discourse at critical points, predicting possible objections and guiding them to particular interpretations with questions, directives and references to shared knowledge \" . The stance the student takes is incremental for the way a student displays its ability to interact with the academic world and display its critical form of thinking. A metadiscourse model was created by Hyland to create a model that displays how writers can express critical thinking, which contains hedging and boosting as two important elements within the creation of stance in writing. Woodward-Kron displays in his work that \" critical thinking \" and other similar statements are incremental in course syllabi which suggest that teachers in academia assign importance to critical thinking in writing courses. Epistemic stance has particularly received quite some attention concerning how undergraduate students employ this to express generality or certainty among their claims in essays and other academic works. A way to incorporate epistemic stance to express less certainty in the writer\\' s stance is hedging. Hedging is the use of epistemic markers to reduce the strength of certain claims or ideas. The problematic part about stance and critical thinking is that the desired way to express stance differs per genre and field of study. As university courses prepare their students to enter the world of academia, this genre is interesting to ensure that the next generation of academics knows how to position and promote themselves within the field and speak the same \" language \" as their peers. As undergraduate writers continue their studies, it is likely that they will improve in their use of stance. It is therefore interesting to compare first and third-year students in their use of stance. Because even though teachers report that they understand and want to improve their students within this particular genre, most results are self-reported . It would, therefore, be fruitful to compare how the students\\' use of stance differs between the first and third years. It seems plausible that exposure to academic sources in cohort with the compulsory writing courses would influence and hopefully improve the way students incorporate epistemic stance into their writing. Based on previous research it would generally be expected for hedging to become relatively more frequent while boosting is used with more care which would result in a lower frequency. The current study provides an adequate small-scale step to see how the knowledge of academic writing and teacher expertise translates into the improvement of undergraduate students throughout their bachelor. Without an actual analysis of the students\\' work, this is nothing more than an assumption and should be compared. The questions that follow from this are as follows : 1 : In what ways, if any, are there differential patterns of epistemic stance expression between the first-year and the third-year papers? 2 : How do the first-year and third-year students\\' patterns reflect their perceived epistemic stance used in the field? This section will provide an overview of the used corpus and the analytical framework that was applied to the corpus. For this research, the corpus collected by the Radboud University will be used to compare literature and culture-based text written by students of English Language and culture. A total of 534 texts were selected for the corpus research of which 406 texts have been selected by first-year students and 128 texts by third years students for the comparison of their use of epistemic stance. All the text were essays of a literary or cultural genre, which was argumentative by nature, which lend itself well to the rhetorically useful hedges and boosters. Epistemic stance will mainly comprise of the students\\' use of boosters and hedges throughout the corpora. For this research, a focus is given to both lexical modal verbs and auxiliary modal verbs for hedges and adverbs for boosters. These are the most frequent forms of epistemic stance and give the best indication through their frequency of the students\\' use of hedging and boosting. Hedges are vital as an epistemic status indicator because of their open-ended status which allows the readers to enter the discursive field and present and create their interpretations of the subject matter at hand . This openness it incremental in the academic discourse due to the interactive and highly debatable and debated content that academia provides. Boosters, on the other hand, provide a form which allows for the writer to express a certainty to display what kind of involvement the author has regarding the topic and the perceived audience. Boosters help effecting interpersonal solidarity and can, together with hedges, provide a system in which the author can display its position regarding statements and claims within their work . The epistemic stance has been researched through relative frequency of hedges and boosters. This measurement was done with the program , which was utilised for the frequency of targeted searches. The list comprises of search items based on the lexical modal verbs and adverbs which fit within the hedging or boosting category. The frequencies are normalised to create a comprehensive overview of the frequency per 10000 word tokens to indicate their relative frequency and consequently their importance, which allows for a more accurate comparison of the two different groups. Following the frequency analysis, the patterns can be scrutinised to see if there are differences between the use of epistemic stance between the first & third years. This comparison allows for a comprehensive overview of the student\\' s usage of epistemic stance and what patterns follow from their education as English majors. The frequency analysis of the lexical modal verbs and the boosters display a similar pattern for both boosters and hedges. As can be seen in table 1, the first-year students had a total frequency of 987 lexical modal verbs as hedges, which was normalised to a 36,93 in the first-year corpus based on 267248 word tokens. It is clear that students have a preference for words such as , and . The results seem to indicate a low amount of variance in the hedge usage of the first-years, which could be attributed to their lack of experience in the field of academic writing. As can be seen in table 2, the adverbs functioning as boosters had a frequency of 520 with a normalised total of 19,46 based on the same first-year corpus. The first-year students show a preference for strong adverbs such as , and . These are, especially in the humanities, difficult words to use unless certain expertise and certainty is implied in a statement. The hedges to boosters rating for the first-years is 1.89 to 1 hedge to booster ratio, which is close to a 2/3 divide in hedges to boosters. When comparing this to Hyland\\' s study of hedges and boosters in different academic disciplines, it can be seen that even though it differed per discipline, on average the number of hedges to boosters was still higher than seen in this corpus. The numbers here suggest that first-year students already use more hedging than boosting in their written discourse albeit with a lower difference in ratio than the papers in Hyland\\' s study. The third years displayed slightly different frequencies and diversity in their use of hedges and boosters. Table 3 displays the use of hedges by the third-year students. They used a total of 896 lexical model verbs as hedges, which was normalised to a 46,49 in the third year corpus consisting of 192725 word tokens. The third-years seem to have a decent distribution among words used, which could indicate that different words are used in different situations, which would display a better awareness of stance usage and a better grip on this particular part of academic writing. As can be seen in table 4, the third-year students\\' corpus contained a total of 449 adverbs as boosters, which was normalised to a 23,30 total in the third-year corpus. The boosters had some cases in which words were hardly used but had a decent distribution in general. Both the hedges and boosters have increased in relative quantity as compared to the first-year students and have a 1.99 to 1 hedge to booster ratio. This result is in line with the notion of Lancaster that a student uses more hedges and relatively fewer boosters as they progress in their studies and increase their knowledge of the field and academic writing in general. There was, however, one thing that seems to differ slightly from Lancaster\\' s results. The total amount of boosters used increased from the first-years to the third-years, which is odd as boosters should be used with care in the academic world and that realisation is expected to result in a decrease of boosters. When comparing the two groups\\' use of hedges, it is clear that the third-year students\\' distribution of hedges is better than their first-year counterpart and contained fewer extreme outliers. Third-years used 27,5% more hedges than the first-year students did, which indicates a better awareness of the options within the academic discourse and the ability to potentially utilise this knowledge. The biggest difference is between for the first-years, which was 35,35% of their total used hedges as compared to 21.88% for the third years. This 60% higher frequency of the first-year students could be a decent indicator of a lack of awareness regarding the academic register and its possibilities to display epistemic stance. Aijmer\\' s corpus research looked at Swedish high-proficiency L2 learners of English and noticed that they frequently used cognition verbs like and . This is also reflected within the first-year\\' s corpus data. , and had higher frequencies by far and were extreme outliers for the first-year students. The third-year students, however, had a remarkable decrease in their use these verbs and improved their register as they learned to write better than the average high-proficiency user. After looking at both first-year writing and third-year writing within the Radboud University\\' s department of English Language & Culture, it is evident that there are differences between the first-year and the third-year students. The corpora results seemed to coincide with studies such as Aijmer\\' s study of Swedish L2 users of English and Aull and Lancaster\\' s study of lower-level and upper-level students. Especially concerning the increase in hedging and the use of cognition verbs to incorporate hedging in their writing. This sample can be used as an indication of the English and Language study\\' s adequacy in teaching its students to make use of epistemic stance. Lancaster noted that the aims in curriculum creation among teachers is often self-reported and should be scrutinised to see if this is adequately articulated based on the students\\' writing. When pertaining to epistemic stance, this seems to be the case for the Radboud University\\' s English Language and Culture department, which is based on the students\\' increase in hedging as well as the better diversity when hedging as they gain experience in the field through their studies. Based on the writing of the third-year students, it could be considered to spend some attention to the metalinguistic use of boosters and their place in the academic texts. The students did learn to write better as they progress through their bachelor curriculum. So, this provides some support for the existence and focus implemented in the writing courses of the Radboud University. The corpus analysis was not without limitations or ambiguities. The problem with simple frequency analysis of certain phenomena is often grounded in the lack of context. This contextual vacuum it creates can make corpus research slightly more difficult due to its lack of distinguishing abilities, which is reflected in words like, for example, modal verbs because they can not account for the deontic use rather than epistemic . It is therefore hard to measure how well the use of hedges is improved when looking at the macro-structure of the text. A discourse analysis of a smaller sample of texts could, however, solve this to see how the hedges and boosters influence the text on macro-scale. Another shortcoming is the small sample of hedges and boosters used. The goal of the research was to compare the frequencies of hedges versus boosters, which could be slightly skewed or inaccurate because not all ways of hedging were incorporated in the research. This would, however, have been problematic for the scope and would decrease the comprehensiveness in an assignment of this size. Future research could try to accommodate these shortcomings by taking the full extent of hedges and boosters available in the academic register and contextualise them. Another idea would be to check the same corpus on different key aspects of academic writing like tense or self-mentions.\\n\\n### Response:\\nDUT</s>'], 'language': ['DUT']}\n"
          ]
        }
      ],
      "source": [
        "alpaca_prompt = '''\n",
        "### Instruction:\n",
        "You are a forensic linguistics expert that reads English texts written by non-native authors to classify the native language of the author as one of:\n",
        "\"DUT\": Dutch\n",
        "\"FRE\": French\n",
        "\"NOR\": Norwegian\n",
        "\"SPA\": Spanish\n",
        "\"SWE\": Swedish\n",
        "Use clues such as spelling errors, word choice, syntactic patterns, and grammatical errors to decide on the native language of the author.\\n\\n\n",
        "\n",
        "DO NOT USE ANY OTHER CLASS.\n",
        "IMPORTANT: Do not classify any input as \"ENG\" (English). English is an invalid choice.\n",
        "\n",
        "Valid output formats:\n",
        "Class: \"DUT\"\n",
        "Class: \"SWE\"\n",
        "Class: \"NOR\"\n",
        "Class: \"SPA\"\n",
        "\n",
        "Classify the text above as one of DUT, FRE, NOR, SPA, or SWE. Do not output any other class - do NOT choose \"ENG\" (English). What is the closest native language of the author of this English text from the given list?\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}'''\n",
        "\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs       = examples[\"text\"]\n",
        "    outputs      = examples[\"language\"]\n",
        "    texts = []\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "vespa_train = \"/content/drive/MyDrive/thesis_NLI/VESPA/VESPA-train.csv\"\n",
        "text_max_seq_length = 7900\n",
        "\n",
        "# random undersampling of training data\n",
        "rus = RandomUnderSampler(sampling_strategy=\"not minority\", random_state=0)\n",
        "df = pd.read_csv(vespa_train)\n",
        "X = df.drop('language',axis=1)\n",
        "y = df['language'].tolist()\n",
        "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "print(len(y_resampled))\n",
        "# print(X_resampled['text'][0])\n",
        "truncated_texts = []\n",
        "lengths = []\n",
        "for text in X_resampled['text'].tolist():\n",
        "  text = truncate_text(text, text_max_seq_length)\n",
        "  truncated_texts.append(text)\n",
        "  length = num_tokens_from_string(text)\n",
        "  lengths.append(length)\n",
        "sample_df = pd.DataFrame({'text': truncated_texts, 'language': y_resampled})\n",
        "print(max(lengths))\n",
        "\n",
        "dataset = Dataset.from_pandas(sample_df)\n",
        "print(Counter(y_resampled))\n",
        "\n",
        "# uncomment the following section for full dataset\n",
        "# from datasets import load_dataset\n",
        "# dataset = load_dataset(\"csv\", data_files=vespa_train, split='train')\n",
        "# print(f'Number of samples: {len(dataset)}')\n",
        "# # print(dataset[0])\n",
        "# texts = dataset['text']\n",
        "# truncated_texts = []\n",
        "# for text in texts:\n",
        "#   text = truncate_text(text, text_max_seq_length)\n",
        "#   truncated_texts.append(text)\n",
        "# df = pd.DataFrame({'text': truncated_texts, 'language': dataset['language']})\n",
        "# dataset = Dataset.from_pandas(df)\n",
        "\n",
        "texts = truncated_texts\n",
        "labels = dataset ['language']\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "print(dataset[:1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "# Train LLM on training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQHREUmjlIMo",
        "outputId": "8a54d0c9-08a6-429a-be77-38f39574ba48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.564 GB.\n",
            "4.5 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c925c615581043dda92154c1fd38ba9a",
            "e4dfb9d7fcf8420b8e7de6bb32ddf6aa",
            "95022b2d7f74487882cc4b535f72281f",
            "1e932f7c6a99409dbb00a7259af55bdd",
            "c9640456c88a4321975d769bb85c4957",
            "075b1ec5ee044995bfc99c001e6c4ea6",
            "34027321353a4077ba4df37e98a7f2bb",
            "32c246645f5e4ea7ad59027513c6b49f",
            "05e51d9894e14426945f0885b2783aad",
            "a98ffcf9de364c778f507db9c9f304c0",
            "caccdc467bb845bfbdd73b60e4b1a40d"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "f53af350-c4f8-4dc4-b7e1-ba4c8fd90abe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/165 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c925c615581043dda92154c1fd38ba9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 165 | Num Epochs = 3\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 16 | Total steps = 30\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 12:40, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.058300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.136800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.119100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.087400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.965500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.055600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.976900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.004300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.961400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.903300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.809200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.874800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.878300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.813100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.824700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.952500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.867600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.907900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.914200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.894100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.926600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.955300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.877200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.885800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.799500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.846400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.909400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.926400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.704600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.899900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/thesis_NLI/VESPA_results/finetuned_mistral_undersampled/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/thesis_NLI/VESPA_results/finetuned_mistral_undersampled/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/thesis_NLI/VESPA_results/finetuned_mistral_undersampled/tokenizer.model',\n",
              " '/content/drive/MyDrive/thesis_NLI/VESPA_results/finetuned_mistral_undersampled/added_tokens.json',\n",
              " '/content/drive/MyDrive/thesis_NLI/VESPA_results/finetuned_mistral_undersampled/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "max_seq_length = 8100\n",
        "\n",
        "for ind, rs in zip(range(1), [100]): # perform 3 runs with 3 different random seeds if necessary\n",
        "  model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hfeds\n",
        "  )\n",
        "  model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = rs,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        "  )\n",
        "  trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4, # The batch size per GPU for training\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "model.save_pretrained(f\"/content/drive/MyDrive/thesis_NLI/VESPA_results/{run_name}\") # Local saving\n",
        "tokenizer.save_pretrained(f\"/content/drive/MyDrive/thesis_NLI/VESPA_results/{run_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "# Evaluate on test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml9oiyz_o1bh"
      },
      "source": [
        ":After fine-tuning, we evaluate the fine-tuned model on a small VESPA test set. We prompt the model to predict the native language for L2 texts in the VESPA test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atu8heYCqb9K"
      },
      "source": [
        "## Defining the functions for running inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei5zdankq_hz"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt):\n",
        "  \"\"\"\n",
        "  Generate text for LLM based on input prompt\n",
        "  :param prompt: input prompt\n",
        "  :param max_length:\n",
        "  :type prompt: str\n",
        "  :type max_length: int\n",
        "  \"\"\"\n",
        "    # Tokenize the prompt\n",
        "  inputs = tokenizer(prompt, max_length = max_seq_length, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "  outputs = model.generate(inputs,\n",
        "                           max_new_tokens=10,\n",
        "                           pad_token_id=tokenizer.eos_token_id,\n",
        "                           #temperature=0.001\n",
        "                           ) # set temperature here?\n",
        "    # Decode the response\n",
        "  response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  return response\n",
        "\n",
        "def clean_output(output, eos_token, output_only=False):\n",
        "  \"\"\"\n",
        "  This function specifically cleans up the output,\n",
        "  to remove the prompt, make sure it is in the correct format, and remove any empty lines.\n",
        "  :param output: model generated output\n",
        "  :param eos_token: end-of-sequence token to split model output on\n",
        "  :param output_only, default False: if True, extract only the newly generated output by model and remove the prompt. mostly used for debugging\n",
        "  :type output: str\n",
        "  :type eos_token: str\n",
        "  \"\"\"\n",
        "  pure_output = output.split(eos_token)\n",
        "  pure_output = pure_output[-1]\n",
        "  pure_output = pure_output.strip()\n",
        "  final_output = pure_output\n",
        "  if output_only==False: # whether to extract only json-formatted string in the output or not\n",
        "    predicted_classes=0\n",
        "    language_class_dict = {'arabic': 'ARA',\n",
        "                            'bulgarian': 'BUL',\n",
        "                             'chinese': 'CHI',\n",
        "                             'czech': 'CZE',\n",
        "                             \"french\": \"FRE\",\n",
        "                             \"german\": \"GER\",\n",
        "                             \"hindi\": \"HIN\",\n",
        "                             \"italian\": \"ITA\",\n",
        "                             \"japanese\": \"JPN\",\n",
        "                             \"korean\": 'KOR',\n",
        "                             \"spanish\": \"SPA\",\n",
        "                             \"telugu\": \"TEL\",\n",
        "                             \"turkish\": \"TUR\",\n",
        "                             \"russian\": \"RUS\",\n",
        "                             \"english\": \"ENG\",\n",
        "                            'sp': 'SPA',\n",
        "                            'itl': 'ITA',\n",
        "                           'deu': 'GER',\n",
        "                           'norwegian': 'NOR',\n",
        "                           'dutch': 'DUT',\n",
        "                           'swedish': 'SWE'\n",
        "                             }\n",
        "    if '}' in final_output:\n",
        "      x = output.split(\"}\")\n",
        "      for piece in x:\n",
        "        if 'native_lang' in piece:\n",
        "          x = piece.split(\":\")\n",
        "          label = x[-1]\n",
        "          label = label.strip()\n",
        "          label = label.replace('\"', '')\n",
        "          label = label.replace('\\n', '')\n",
        "          final_output = '{\"native_lang\":\"' + label + '\"}'\n",
        "    if 'Class:' in final_output:\n",
        "      x = output.split(\"Class:\")\n",
        "      label = x[-1]\n",
        "      label = label.strip()\n",
        "      label = label.replace('\"', '')\n",
        "      label = label.replace('\\n', '')\n",
        "      label = label.replace('.', '')\n",
        "      final_output = label\n",
        "    # print(len(final_output))\n",
        "    if len(final_output)>50:\n",
        "      final_output = '{\"native_lang\": \"unknown\"}'\n",
        "    else:\n",
        "      for lang, label in language_class_dict.items():\n",
        "        if lang in final_output.lower() or label in final_output:\n",
        "          final_output = '{\"native_lang\":\"' + label + '\"}'\n",
        "  return final_output\n",
        "\n",
        "def classify(texts, goldlabels, filter_token):\n",
        "  '''\n",
        "  :param texts: list of texts\n",
        "  :param goldlabels: list of gold labels\n",
        "  :param filter_token: token to get cleaned output\n",
        "  :type texts: list\n",
        "  :type goldlabels: list\n",
        "  :returns predictions: a list of model predictions\n",
        "  '''\n",
        "  predictions = []\n",
        "  count = 1\n",
        "  sys_prompt = prompt_VESPA\n",
        "  prompt_retry = prompt_retry_VESPA\n",
        "  all_labels = all_labels_VESPA\n",
        "  NLI_prediction = NLI_prediction_VESPA\n",
        "  main_task_prompt = main_task_prompt_VESPA\n",
        "  for text, gold in zip(texts, goldlabels):\n",
        "    promptcounter = 0\n",
        "    # num_tokens = num_tokens_from_string(text, encoder)\n",
        "    # if num_tokens>3000:\n",
        "    text = truncate_text(text, 7900)\n",
        "    while True:\n",
        "      try:\n",
        "        fullprompt = \"Instruction: \" + sys_prompt + '\\n\\n' + main_task_prompt + '\\n\\nInput: '+ text + \"\\nResponse:\"\n",
        "        #print(fullprompt)\n",
        "        output = generate_text(fullprompt) # generate text per TOEFL text\n",
        "        # print(output)\n",
        "        output_only = clean_output(output, filter_token, output_only=True)\n",
        "        print(output_only)\n",
        "        final_output = clean_output(output, filter_token)\n",
        "        validated_response = NLI_prediction.model_validate_json(final_output) # use class to validate json string\n",
        "        response_dict = validated_response.model_dump() # dump validated response into dict\n",
        "        predicted_native_lang = response_dict['native_lang'] # get the predicted native language\n",
        "        if predicted_native_lang == \"ENG\": # reiterate prompt if model predicts english\n",
        "          fullprompt = \"Instruction: \" + sys_prompt + '\\n\\n' + main_task_prompt + '\\n' + prompt_retry_eng + '\\n\\nInput: '+ text + \"\\nResponse:\"\n",
        "          promptcounter+=1\n",
        "          if promptcounter > 4: # try 5 times to reprompt, if still unable to extract predicted label, append other\n",
        "            response_dict = {'native_lang': 'other'}\n",
        "            predictions.append('other')\n",
        "            break\n",
        "        predictions.append(predicted_native_lang) # append it to list of predictions\n",
        "        break\n",
        "      # print(final_output)\n",
        "      except ValidationError as e: # if there is a validation error, make model retry\n",
        "        fullprompt = \"Instruction: \" + sys_prompt + '\\n\\n' + main_task_prompt + '\\n' + prompt_retry + '\\n\\nInput: '+ text + \"\\nResponse:\"\n",
        "        promptcounter +=1\n",
        "        if promptcounter > 4: # try 5 times to reprompt, if still unable to extract predicted label, append other\n",
        "          response_dict = {'native_lang': 'other'}\n",
        "          predictions.append('other')\n",
        "          break\n",
        "    print(count, response_dict)\n",
        "    print('Accuracy score:', \"{:.2f}\".format(accuracy_score(goldlabels[0:count], predictions)))\n",
        "    count +=1\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQAPr_otqRcq"
      },
      "source": [
        "## Defining the prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLjH_5xVr1sb"
      },
      "outputs": [],
      "source": [
        "class NLI_prediction_VESPA(BaseModel):\n",
        "  native_lang: Literal['DUT', 'FRE', 'NOR', 'SWE', 'SPA', 'ENG']\n",
        "\n",
        "all_labels_VESPA = ['DUT', 'FRE', 'NOR', 'SWE', 'SPA']\n",
        "\n",
        "vespa_dataset = f\"/content/drive/MyDrive/thesis_NLI/VESPA/VESPA-test.csv\"\n",
        "dataset = pd.read_csv(vespa_dataset)\n",
        "test_texts = dataset['text'].tolist()\n",
        "test_labels = dataset ['language'].tolist()\n",
        "prompt_VESPA = '''\n",
        "  You are a forensic linguistics expert that reads English texts written by non-native authors to classify the native language of the author as one of:\n",
        "  \"DUT\": Dutch\n",
        "  \"FRE\": French\n",
        "  \"NOR\": Norwegian\n",
        "  \"SPA\": Spanish\n",
        "  \"SWE\": Swedish\n",
        "  Use clues such as spelling errors, word choice, syntactic patterns, and grammatical errors to decide on the native language of the author.\\n\\n\n",
        "\n",
        "  DO NOT USE ANY OTHER CLASS.\n",
        "  IMPORTANT: Do not classify any input as \"ENG\" (English). English is an invalid choice.\n",
        "\n",
        "  Valid output formats:\n",
        "  Class: \"DUT\"\n",
        "  Class: \"SWE\"\n",
        "  Class: \"NOR\"\n",
        "  Class: \"SPA\"\n",
        "\n",
        "  You ONLY respond in JSON files.\n",
        "  The expected output from you has to be:\"json {\"native_lang\": The chosen class, DUT, FRE, NOR, SPA, or SWE}\"\n",
        "'''\n",
        "\n",
        "main_task_prompt_VESPA = '''Classify the text above as one of DUT, FRE, NOR, SPA, or SWE. Do not output any other class - do NOT choose \"ENG\" (English). What is the closest native language of the author of this English text from the given list?\n",
        "'''\n",
        "\n",
        "prompt_retry_eng = '''\n",
        "  You previously mistakenly predicted this text as \"ENG\" (English). The class is NOT English.\n",
        "  Please classify the native language of the author of the text again.\n",
        "  '''\n",
        "\n",
        "prompt_retry_VESPA = '''\n",
        "  Your classification is not in the list of possible languages.\n",
        "  Please try again and choose only one of the following classes:\n",
        "  DUT, FRE, NOR, SPA, or SWE\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNewwXg-qivE"
      },
      "source": [
        "## NLI classification using fine-tuned LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUvVwofMsUqL"
      },
      "outputs": [],
      "source": [
        "all_models = ['finetuned_llama2_7b', 'finetuned_llama3_8b', 'finetuned_mistral_7b', 'finetuned_gemma_7b, finetuned_phi3']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "accuracies = []\n",
        "%cd /content/drive/MyDrive/thesis_NLI/VESPA_results\n",
        "runs = 1\n",
        "# for count in range(runs):\n",
        "count=0\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "      model_name = run_name, # YOUR MODEL YOU USED FOR TRAINING\n",
        "      max_seq_length = max_seq_length,\n",
        "      dtype = dtype,\n",
        "      load_in_4bit = load_in_4bit,\n",
        "  )\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "eos_token = 'Response:'\n",
        "predictions = classify(test_texts, test_labels, eos_token)\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "accuracy2 = \"{:.2f}\".format(accuracy*100)\n",
        "\n",
        "print(f'-------------Run: {count}')\n",
        "print(f'-------------Accuracy: {accuracy2}')\n",
        "accuracy=float(accuracy*100)\n",
        "accuracies.append(accuracy)\n",
        "cm = confusion_matrix(test_labels, predictions, labels=all_labels_VESPA)\n",
        "cm_display = ConfusionMatrixDisplay(cm, display_labels=all_labels_VESPA).plot()\n",
        "cm_display.figure_.savefig(f'/content/drive/MyDrive/thesis_NLI/VESPA_results/{run_name}.png')\n",
        "df = pd.read_csv(vespa_dataset)\n",
        "column_name = f'{run_name}'\n",
        "if column_name in df.columns.values.tolist():\n",
        "  df.pop(column_name)\n",
        "num_columns = len(df.columns)\n",
        "df.insert(num_columns, column_name, predictions)\n",
        "    # df.head()\n",
        "df.to_csv(vespa_dataset, index=False)\n",
        "\n",
        "avg_acc=sum(accuracies)/runs\n",
        "print(accuracies)\n",
        "print(f\"Average acc: {avg_acc}\")\n",
        "print(f\"Standard deviation: {np.std(accuracies)}\") #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4CbLVU4CjTg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "885af4dda23246428187bf4cf13f32c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a65919859e4d4b8c8ba4c16d1f3aa328",
              "IPY_MODEL_03bf1fcbadb5468fafe2c0b596aeb590",
              "IPY_MODEL_5950326afc4442eda287f17ad81a63fb"
            ],
            "layout": "IPY_MODEL_ffdc9f17bf9e4cb081f8b1d1a43d661d"
          }
        },
        "a65919859e4d4b8c8ba4c16d1f3aa328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d869449b629a4ca880bf452a130ed586",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_703736c8fe5c4a01938ca58bb5f61535",
            "value": "Map:â€‡100%"
          }
        },
        "03bf1fcbadb5468fafe2c0b596aeb590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df1becd2ed9b426fa4b2d2c05d411861",
            "max": 165,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffd55ed65361436c93d471f53564ff9c",
            "value": 165
          }
        },
        "5950326afc4442eda287f17ad81a63fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_994f0b6504134bff8cd091b3eeb4e376",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_677d2b2799d64008bdb05b4a7767564a",
            "value": "â€‡165/165â€‡[00:00&lt;00:00,â€‡6196.09â€‡examples/s]"
          }
        },
        "ffdc9f17bf9e4cb081f8b1d1a43d661d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d869449b629a4ca880bf452a130ed586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "703736c8fe5c4a01938ca58bb5f61535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df1becd2ed9b426fa4b2d2c05d411861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffd55ed65361436c93d471f53564ff9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "994f0b6504134bff8cd091b3eeb4e376": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "677d2b2799d64008bdb05b4a7767564a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c925c615581043dda92154c1fd38ba9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4dfb9d7fcf8420b8e7de6bb32ddf6aa",
              "IPY_MODEL_95022b2d7f74487882cc4b535f72281f",
              "IPY_MODEL_1e932f7c6a99409dbb00a7259af55bdd"
            ],
            "layout": "IPY_MODEL_c9640456c88a4321975d769bb85c4957"
          }
        },
        "e4dfb9d7fcf8420b8e7de6bb32ddf6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_075b1ec5ee044995bfc99c001e6c4ea6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_34027321353a4077ba4df37e98a7f2bb",
            "value": "Mapâ€‡(num_proc=2):â€‡100%"
          }
        },
        "95022b2d7f74487882cc4b535f72281f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32c246645f5e4ea7ad59027513c6b49f",
            "max": 165,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05e51d9894e14426945f0885b2783aad",
            "value": 165
          }
        },
        "1e932f7c6a99409dbb00a7259af55bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a98ffcf9de364c778f507db9c9f304c0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_caccdc467bb845bfbdd73b60e4b1a40d",
            "value": "â€‡165/165â€‡[00:01&lt;00:00,â€‡67.34â€‡examples/s]"
          }
        },
        "c9640456c88a4321975d769bb85c4957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "075b1ec5ee044995bfc99c001e6c4ea6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34027321353a4077ba4df37e98a7f2bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32c246645f5e4ea7ad59027513c6b49f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05e51d9894e14426945f0885b2783aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a98ffcf9de364c778f507db9c9f304c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caccdc467bb845bfbdd73b60e4b1a40d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}