{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSHlAbqzDFDq"
      },
      "source": [
        "# Running Open-Source LLMs for Native Language Identification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3USAIUqul9h"
      },
      "source": [
        "### Loading Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAMzy_0FtaUZ",
        "outputId": "422a8dbb-93ae-4941-8642-4e817056872f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from random import randrange\n",
        "from functools import partial\n",
        "import torch\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          HfArgumentParser,\n",
        "                          Trainer,\n",
        "                          TrainingArguments,\n",
        "                          DataCollatorForLanguageModeling,\n",
        "                          EarlyStoppingCallback,\n",
        "                          pipeline,\n",
        "                          logging,\n",
        "                          set_seed)\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "from pydantic import BaseModel, ValidationError, Field # used for validation of the output\n",
        "from typing import Literal\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import LinearSVC\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "from unsloth import FastLanguageModel\n",
        "import numpy as np\n",
        "# from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UPJHejW8w_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a730a5-64e6-4a0f-e42a-665c33a2a580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-yR2ZVz2lHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f2279a-d691-48c9-eb24-968e6a462911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login # make use of your own huggingface token here\n",
        "# Llama models require you to fill out an agreement form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIqPR8O-s7WE"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name, bnb_config):\n",
        "    \"\"\"\n",
        "    Loads model and model tokenizer\n",
        "\n",
        "    :param model_name: Hugging Face model\n",
        "    :param bnb_config: Bitsandbytes configuration\n",
        "    \"\"\"\n",
        "\n",
        "    # Get number of GPU devices available and set maximum memory\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    max_memory = f'{40960}MB'\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config = bnb_config,\n",
        "        device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
        "        max_memory = {i: max_memory for i in range(n_gpus)}, # max memory for gpus available\n",
        "        torch_dtype = torch.float16\n",
        "    )\n",
        "\n",
        "    # Load model tokenizer with the user authentication token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True, trust_remote_code=True)\n",
        "\n",
        "    # Set padding token as end of sequence (EOS) token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQsdgLh44yM-"
      },
      "source": [
        "### Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gEb5z2j5FyS"
      },
      "outputs": [],
      "source": [
        "# Filepaths to datasets\n",
        "BASEPATH = \"/content/drive/MyDrive/thesis_NLI/\"\n",
        "toefl_dataset = f\"{BASEPATH}TOEFL11/toefl_results.csv\"\n",
        "icle_dataset = f\"{BASEPATH}ICLE-NLI-results.csv\"\n",
        "vespa_dataset = f\"{BASEPATH}/VESPA/VESPA-test.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLoaZvdTmIZT",
        "outputId": "f53b2cdb-548f-40ec-9909-8c6d81299fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 50\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "dataset = pd.read_csv(vespa_dataset) # change this to ICLE/TOEFL/VESPA accordingly\n",
        "print(f'Number of samples: {len(dataset)}')\n",
        "# print(f'Column names are: {list(dataset.columns)}')\n",
        "texts = dataset['text'].tolist()\n",
        "labels = dataset ['language'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47tUZmTQieUr"
      },
      "outputs": [],
      "source": [
        "class NLI_prediction_TOEFL(BaseModel):\n",
        "  native_lang: Literal['ARA', 'CHI', 'FRE', 'GER', 'HIN', 'ITA', 'JPN', 'KOR', 'SPA', 'TEL', 'TUR', 'ENG']\n",
        "\n",
        "all_labels_TOEFL = ['ARA', 'CHI', 'FRE', 'GER', 'HIN', 'ITA', 'JPN', 'KOR', 'SPA', 'TEL', 'TUR']\n",
        "\n",
        "class NLI_prediction_ICLE(BaseModel):\n",
        "  native_lang: Literal['BUL', 'CHI', 'CZE', 'FRE', 'JPN', 'RUS', 'SPA', 'ENG']\n",
        "\n",
        "all_labels_ICLE = ['BUL', 'CHI', 'CZE', 'FRE', 'JPN', 'RUS', 'SPA']\n",
        "\n",
        "class NLI_prediction_VESPA(BaseModel):\n",
        "  native_lang: Literal['DUT', 'FRE', 'NOR', 'SWE', 'SPA', 'ENG']\n",
        "\n",
        "all_labels_VESPA = ['DUT', 'FRE', 'NOR', 'SWE', 'SPA']\n",
        "\n",
        "all_labels_TOEFL_open = ['ARA', 'CHI', 'FRE', 'GER', 'HIN', 'ITA', 'JPN', 'KOR', 'SPA', 'TEL', 'TUR', 'ENG', 'other']\n",
        "all_labels_ICLE_open = ['BUL', 'CHI', 'CZE', 'FRE', 'JPN', 'RUS', 'SPA', 'ENG', 'other']\n",
        "all_labels_VESPA_open = ['DUT', 'FRE', 'NOR', 'SWE', 'SPA', 'ENG', 'other']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4cqyOJ_Sl8f"
      },
      "source": [
        "# Prompting LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPirQSz5Ipbp"
      },
      "source": [
        "## Defining the prompts for closed-class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWXIOnmab5UY"
      },
      "outputs": [],
      "source": [
        "prompt_TOEFL = '''\n",
        "  You are a forensic linguistics expert that reads English texts written by non-native authors to classify the native language of the author as one of:\n",
        "  \"ARA\": Arabic\n",
        "  \"CHI\": Chinese\n",
        "  \"FRE\": French\n",
        "  \"GER\": German\n",
        "  \"HIN\": Hindi\n",
        "  \"ITA\": Italian\n",
        "  \"JPN\": Japanese\n",
        "  \"KOR\": Korean\n",
        "  \"SPA\": Spanish\n",
        "  \"TEL\": Telugu\n",
        "  \"TUR\": Turkish\n",
        "  Use clues such as spelling errors, word choice, syntactic patterns, and grammatical errors to decide on the native language of the author.\\n\n",
        "\n",
        "  DO NOT USE ANY OTHER CLASS.\n",
        "  IMPORTANT: Do not classify any input as \"ENG\" (English). English is an invalid choice.\n",
        "\n",
        "  Valid output formats:\n",
        "  Class: \"ARA\"\n",
        "  Class: \"CHI\"\n",
        "  Class: \"FRE\"\n",
        "  Class: \"GER\"\n",
        "\n",
        "  You ONLY respond in JSON files.\n",
        "  The expected output from you is:\"json {\"native_lang\": The chosen class, ARA, CHI, FRE, GER, HIN, ITA, JPN, KOR, SPA, TEL, or TUR}\"\n",
        "  '''\n",
        "\n",
        "prompt_ICLE = '''\n",
        "  You are a forensic linguistics expert that reads English texts written by non-native authors to classify the native language of the author as one of:\n",
        "  \"BUL\": Bulgarian\n",
        "  \"CHI\": Chinese\n",
        "  \"CZE\": Czech\n",
        "  \"FRE\": French\n",
        "  \"JPN\": Japanese\n",
        "  \"RUS\": Russian\n",
        "  \"SPA\": Spanish\n",
        "  Use clues such as spelling errors, word choice, syntactic patterns, and grammatical errors to decide on the native language of the author.\\n\\n\n",
        "\n",
        "  DO NOT USE ANY OTHER CLASS.\n",
        "  IMPORTANT: Do not classify any input as \"ENG\" (English). English is an invalid choice.\n",
        "\n",
        "  Valid output formats:\n",
        "  Class: \"BUL\"\n",
        "  Class: \"CHI\"\n",
        "  Class: \"CZE\"\n",
        "  Class: \"SPA\"\n",
        "\n",
        "  You ONLY respond in JSON files.\n",
        "  The expected output from you has to be:\"json {\"native_lang\": The chosen class, BUL, CHI, CZE, FRE, JPN, RUS, or SPA}\"\n",
        "  '''\n",
        "\n",
        "prompt_VESPA = '''\n",
        "  You are a forensic linguistics expert that reads English texts written by non-native authors to classify the native language of the author as one of:\n",
        "  \"DUT\": Dutch\n",
        "  \"FRE\": French\n",
        "  \"NOR\": Norwegian\n",
        "  \"SPA\": Spanish\n",
        "  \"SWE\": Swedish\n",
        "  Use clues such as spelling errors, word choice, syntactic patterns, and grammatical errors to decide on the native language of the author.\\n\\n\n",
        "\n",
        "  DO NOT USE ANY OTHER CLASS.\n",
        "  IMPORTANT: Do not classify any input as \"ENG\" (English). English is an invalid choice.\n",
        "\n",
        "  Valid output formats:\n",
        "  Class: \"DUT\"\n",
        "  Class: \"SWE\"\n",
        "  Class: \"NOR\"\n",
        "  Class: \"SPA\"\n",
        "\n",
        "  You ONLY respond in JSON files.\n",
        "  The expected output from you has to be:\"json {\"native_lang\": The chosen class, DUT, FRE, NOR, SPA, or SWE}\"\n",
        "'''\n",
        "\n",
        "main_task_prompt_TOEFL = '''Classify the text below as one of ARA, CHI, FRE, GER, HIN, ITA, JPN, KOR, SPA, TEL, or TUR. Do not output any other class - do NOT choose \"ENG\" (English). What is the closest native language of the author of this English text from the given list?\n",
        "'''\n",
        "\n",
        "main_task_prompt_ICLE = '''Classify the text below as one of BUL, CHI, CZE, FRE, JPN, RUS, or SPA. Do not output any other class - do NOT choose \"ENG\" (English). What is the closest native language of the author of this English text from the given list?\n",
        "'''\n",
        "\n",
        "main_task_prompt_VESPA = '''Classify the text below as one of DUT, FRE, NOR, SPA, or SWE. Do not output any other class - do NOT choose \"ENG\" (English). What is the closest native language of the author of this English text from the given list? Provide ONE language even if you are unsure.\n",
        "'''\n",
        "\n",
        "\n",
        "prompt_retry_eng = '''\n",
        "  You previously mistakenly predicted this text as \"ENG\" (English). The class is NOT English.\n",
        "  Please classify the native language of the author of the text again.\n",
        "  '''\n",
        "\n",
        "prompt_retry_TOEFL = '''\n",
        "  Your classification is not in the list of possible languages.\n",
        "  Please try again and choose only one of the following classes:\n",
        "  ARA, CHI, FRE, GER, HIN, ITA, JPN, KOR, SPA, TEL, or TUR\n",
        "  '''\n",
        "\n",
        "prompt_retry_ICLE = '''\n",
        "  Your classification is not in the list of possible languages.\n",
        "  Please try again and choose only one of the following classes:\n",
        "  BUL, CHI, CZE, FRE, JPN, RUS, or SPA\n",
        "  '''\n",
        "\n",
        "prompt_retry_VESPA = '''\n",
        "  Your classification is not in the list of possible languages.\n",
        "  Please try again and choose only one of the following classes:\n",
        "  DUT, FRE, NOR, SPA, or SWE\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdKcXAbjvVU5"
      },
      "source": [
        "## Defining the prompts for open class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2sNwff0vZzq"
      },
      "outputs": [],
      "source": [
        "class NLI_prediction_open(BaseModel):\n",
        "  native_lang: str\n",
        "\n",
        "nli_prediction_dict = NLI_prediction_open.model_json_schema()\n",
        "nli_prediction_json = json.dumps(nli_prediction_dict, indent=2)\n",
        "\n",
        "prompt_open = '''\n",
        "You are a forensic linguistics expert that reads texts written by non-native authors in order to identify their native language.\n",
        "\n",
        "Analyze each text and identify the native language of the author.\n",
        "\n",
        "Use clues such as spelling errors, word choice, syntactic patterns, and grammatical errors to decide.\n",
        "\n",
        "You ONLY respond in JSON files.\n",
        "The expected output from you has to be:\n",
        "\n",
        "json {\"native_lang\": \"\"}\n",
        "'''\n",
        "\n",
        "prompt_retry_open = '''\n",
        "Your previous classification was not in the correct format. Please only respond in the following JSON format:\n",
        "json {\"native_lang\": \"\"}\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ruxtJgSc0h"
      },
      "source": [
        "## Defining functions for prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrGagyPJ9Gcc"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt):\n",
        "  \"\"\"\n",
        "  Generate text for LLM based on input prompt\n",
        "  :param prompt: input prompt\n",
        "  :type prompt: str\n",
        "  \"\"\"\n",
        "    # Tokenize the prompt\n",
        "  inputs = tokenizer(prompt, max_length = 8000, truncation = True, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "  outputs = model.generate(inputs,\n",
        "                           pad_token_id=tokenizer.eos_token_id,\n",
        "                           max_new_tokens=60,\n",
        "                           )\n",
        "    # Decode the response\n",
        "  response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7iFbJ_4CbHU"
      },
      "outputs": [],
      "source": [
        "def clean_output(output, eos_token, output_only=False):\n",
        "  \"\"\"\n",
        "  This function cleans up the output by LLM to remove the prompt, make sure it is in the correct format, and remove any empty lines.\n",
        "  :param output: LLM-generated output\n",
        "  :param eos_token: end-of-sequence token to split model output on\n",
        "  :param output_only, default False: if True, extract only the newly generated output by model and remove the prompt\n",
        "  :type output: str\n",
        "  :type labels: list\n",
        "  :type eos_token: str\n",
        "  \"\"\"\n",
        "  pure_output = output.split(eos_token)\n",
        "  pure_output = pure_output[-1]\n",
        "  pure_output = pure_output.strip()\n",
        "  final_output = pure_output\n",
        "  if output_only==False: # whether to extract only json-formatted string in the output or not\n",
        "    predicted_classes=0\n",
        "    language_class_dict = {'arabic': 'ARA',\n",
        "                            'bulgarian': 'BUL',\n",
        "                             'chinese': 'CHI',\n",
        "                             'czech': 'CZE',\n",
        "                             \"french\": \"FRE\",\n",
        "                             \"german\": \"GER\",\n",
        "                             \"hindi\": \"HIN\",\n",
        "                             \"italian\": \"ITA\",\n",
        "                             \"japanese\": \"JPN\",\n",
        "                             \"korean\": 'KOR',\n",
        "                             \"spanish\": \"SPA\",\n",
        "                             \"telugu\": \"TEL\",\n",
        "                             \"turkish\": \"TUR\",\n",
        "                             \"russian\": \"RUS\",\n",
        "                            #  \"english\": \"ENG\",\n",
        "                             \"itl\": \"ITA\",\n",
        "                             \"deu\": \"GER\",\n",
        "                            \"fra\": 'FRE',\n",
        "                            'dutch': 'DUT',\n",
        "                            'norwegian': 'NOR',\n",
        "                            'swedish': 'SWE',\n",
        "                             }\n",
        "    if '}' in final_output:\n",
        "      x = output.split(\"}\")\n",
        "      for piece in x:\n",
        "        if 'native_lang' in piece:\n",
        "          x = piece.split(\":\")\n",
        "          label = x[-1]\n",
        "          label = label.strip()\n",
        "          label = label.replace('\"', '')\n",
        "          label = label.replace('\\n', '')\n",
        "          final_output = '{\"native_lang\":\"' + label + '\"}'\n",
        "    if 'Class:**' in final_output:\n",
        "      x = output.split(\"Class:**\")\n",
        "      label = x[-1]\n",
        "      label = label.strip()\n",
        "      label = label.replace('\"', '')\n",
        "      label = label.replace('\\n', '')\n",
        "      label = label.replace('.', '')\n",
        "      label = label.replace('*', '')\n",
        "      final_output = label\n",
        "    for lang, label in language_class_dict.items():\n",
        "      if lang in final_output.lower():\n",
        "        final_output = '{\"native_lang\":\"' + label + '\"}'\n",
        "      elif label in final_output:\n",
        "        final_output = '{\"native_lang\":\"' + label + '\"}'\n",
        "\n",
        "  return final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TjNdhaadxsK"
      },
      "source": [
        "## Classification functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvNc0Vij9MhM"
      },
      "outputs": [],
      "source": [
        "def classify(texts, goldlabels, dataset, filter_token, closedopen_setting=\"closed\", openai_api=False):\n",
        "  '''\n",
        "  :param texts: list of texts\n",
        "  :param goldlabels: list of gold labels\n",
        "  :param dataset: TOEFL or ICLE\n",
        "  :param sos_token: start-of-sequence token\n",
        "  :param sys_token: start of system prompt token\n",
        "  :param sys_end_token: end of system prompt token\n",
        "  :param eos_token: end-of-sequence token\n",
        "  :param filter_token: token to get cleaned output\n",
        "  :type texts: list\n",
        "  :type goldlabels: list\n",
        "  :type dataset: str\n",
        "  :returns predictions: a list of model predictions\n",
        "  '''\n",
        "  predictions = []\n",
        "  if dataset == \"TOEFL\": # determine which dataset it is, because the prompts and classes are different for each\n",
        "    sys_prompt = prompt_TOEFL\n",
        "    prompt_retry = prompt_retry_TOEFL\n",
        "    all_labels = all_labels_TOEFL\n",
        "    NLI_prediction = NLI_prediction_TOEFL\n",
        "    main_task_prompt = main_task_prompt_TOEFL\n",
        "  elif dataset == \"ICLE\":\n",
        "    sys_prompt = prompt_ICLE\n",
        "    prompt_retry = prompt_retry_ICLE\n",
        "    all_labels = all_labels_ICLE\n",
        "    NLI_prediction = NLI_prediction_ICLE\n",
        "    main_task_prompt = main_task_prompt_ICLE\n",
        "  elif dataset == 'VESPA':\n",
        "    sys_prompt = prompt_VESPA\n",
        "    prompt_retry = prompt_retry_VESPA\n",
        "    all_labels = all_labels_VESPA\n",
        "    NLI_prediction = NLI_prediction_VESPA\n",
        "    main_task_prompt = main_task_prompt_VESPA\n",
        "  count = 1\n",
        "  if closedopen_setting == 'open':\n",
        "    main_task_prompt = ''\n",
        "    sys_prompt = prompt_open\n",
        "    NLI_prediction = NLI_prediction_open\n",
        "  for text, gold in zip(texts, goldlabels):\n",
        "    promptcounter = 0\n",
        "    main_task = main_task_prompt + '\\n\\n' + text\n",
        "    # messages = [\n",
        "        # {'role': \"system\", \"content\": sys_prompt},\n",
        "        # {'role': \"user\", \"content\": main_task}]\n",
        "    messages = [{'role': \"user\", \"content\": sys_prompt + \"\\n\" +main_task }]\n",
        "    while True:\n",
        "      try:\n",
        "        if openai_api==True:\n",
        "          response = client.chat.completions.create(\n",
        "              model=model,\n",
        "              messages=messages,\n",
        "              seed=7)\n",
        "          output_only = response.choices[0].message.content\n",
        "        else:\n",
        "          # fullprompt = \"Instruction: \" + sys_prompt + '\\n\\n' + main_task_prompt + '\\n\\nInput: '+ text + \"\\nResponse:\"\n",
        "          fullprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "          output = generate_text(fullprompt) # generate text per TOEFL text\n",
        "          output_only = clean_output(output, filter_token, output_only=True)\n",
        "          # print(fullprompt)\n",
        "          print(output_only)\n",
        "          final_output = clean_output(output, filter_token)\n",
        "        #print(final_output)\n",
        "        # print(final_output)\n",
        "        validated_response = NLI_prediction.model_validate_json(final_output) # use class to validate json string\n",
        "        response_dict = validated_response.model_dump() # dump validated response into dict\n",
        "        predicted_native_lang = response_dict['native_lang'] # get the predicted native language\n",
        "        if predicted_native_lang == \"ENG\" and closedopen_setting==\"closed\": # reiterate prompt if model predicts english\n",
        "          # print(final_output)\n",
        "          # fullprompt = \"Instruction: \" + sys_prompt + '\\n\\n' + main_task_prompt + '\\n' + prompt_retry_eng + '\\n\\nInput: '+ text + \"\\nResponse:\"\n",
        "          messages.append({'role': 'assistant', 'content': output_only})\n",
        "          messages.append({'role': \"user\", \"content\": prompt_retry_eng})\n",
        "          # fullprompt = \"Context: \" + sys_prompt + text + '\\nQuestion: '+ main_task_prompt + prompt_retry_eng + \"\\nAnswer:\"\n",
        "          promptcounter+=1\n",
        "          if promptcounter > 4: # try 5 times to reprompt, if still unable to extract predicted label, append other\n",
        "            response_dict = {'native_lang': 'other'}\n",
        "            predictions.append('other')\n",
        "            break\n",
        "        else:\n",
        "          predictions.append(predicted_native_lang) # append it to list of predictions\n",
        "          break\n",
        "      # print(final_output)\n",
        "      except ValidationError as e: # if there is a validation error, make model retry\n",
        "        messages.append({'role': 'assistant', 'content': output_only})\n",
        "        messages.append({'role': \"user\", \"content\": prompt_retry})\n",
        "        # fullprompt = \"Instruction: \" + sys_prompt + '\\n\\n' + main_task_prompt + '\\n' + prompt_retry + '\\n\\nInput: '+ text + \"\\nResponse:\"\n",
        "        promptcounter +=1\n",
        "        if promptcounter > 4: # try 5 times to reprompt, if still unable to extract predicted label, append other\n",
        "          response_dict = {'native_lang': 'other'}\n",
        "          predictions.append('other')\n",
        "          break\n",
        "    print(count, response_dict)\n",
        "    print('Accuracy:', \"{:.2f}\".format(accuracy_score(goldlabels[0:count], predictions)*100))\n",
        "    count +=1\n",
        " #   if count%110 == 0: # save results every 110 samples\n",
        " #     add_results_to_csv(results_filepath, results_column, predictions)\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--Bx3OIIGra2"
      },
      "outputs": [],
      "source": [
        "def add_results_to_csv(filepath, column_name, predictions):\n",
        "  \"\"\"\n",
        "  Adds model predictions as the last column to existing CSV file\n",
        "  with the texts and gold labels of the dataset.\n",
        "\n",
        "\n",
        "  :param filepath: the path to existing CSV file\n",
        "  :param column_name: name of the new column (something like preds_zero_llama7b)\n",
        "  :param predictions: model predictions\n",
        "  :type filepath: str\n",
        "  :type column_name: str\n",
        "  :type predictions: list\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(filepath)\n",
        "  if column_name in df.columns.tolist():\n",
        "    df = df.drop(column_name, axis=1)\n",
        "  if int(len(predictions)) != int(len(df)):\n",
        "    num_empty_predictions = int(len(df))-int(len(predictions))\n",
        "    empty_l = [\"\"] * num_empty_predictions\n",
        "    predictions = predictions+empty_l\n",
        "  num_columns = len(df.columns)\n",
        "  df.insert(num_columns, column_name, predictions)\n",
        "  df.to_csv(filepath, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA6HsLA_LI9B"
      },
      "outputs": [],
      "source": [
        "def evaluate(filepath, labels, pred_column, save_figure=True):\n",
        "  \"\"\"\n",
        "  Print confusion matrix and F1 score for model predictions.\n",
        "  Derives gold labels and predictions from a CSV file,\n",
        "  that contains a column with gold labels that should be called 'language' and predictions from a column specified.\n",
        "\n",
        "  :param filepath: the path to existing CSV file\n",
        "  :param labels: a list of all labels/classes\n",
        "  :param pred_column: the name of the column that contains the predictions\n",
        "  :type filepath: str\n",
        "  :type labels: list\n",
        "  :type pred_column: str\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(filepath, on_bad_lines='warn')\n",
        "  gold_labels = df['language'].tolist()\n",
        "  predictions = df[pred_column].tolist()\n",
        "  cm = confusion_matrix(gold_labels, predictions, labels=labels)\n",
        "  cm_display = ConfusionMatrixDisplay(cm, display_labels=labels).plot(cmap='Blues')\n",
        "  if save_figure == True:\n",
        "    if 'ICLE' in filepath:\n",
        "      ds='ICLE'\n",
        "    elif 'TOEFL' in filepath:\n",
        "      ds='TOEFL'\n",
        "    elif 'VESPA' in filepath:\n",
        "      ds = 'VESPA'\n",
        "    cm_display.figure_.savefig(f'/content/drive/MyDrive/thesis_NLI/{ds}_results/{pred_column}.png')\n",
        "  f1score = \"{:.4f}\".format(f1_score(gold_labels, predictions, average=\"macro\"))\n",
        "  accuracy = accuracy_score(gold_labels, predictions)\n",
        "  accuracy = \"{:.2f}\".format(accuracy*100)\n",
        "  print(pred_column)\n",
        "  print('F1: ' + str(f1score) + '\\nAccuracy: ' + str(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running inference"
      ],
      "metadata": {
        "id": "0FyiGa_jpzt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_models = [\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "              \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "              \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "              \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "              \"google/gemma-7b-it\"\n",
        "              ]"
      ],
      "metadata": {
        "id": "taJRCxAdpu_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V9mDXu1g-Y4"
      },
      "source": [
        "## Llama for NLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "542r2h250cMN"
      },
      "source": [
        "### Loading and prompting Llama models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pb6Q94ZttDEB"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "  # model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_compute_dtype=torch.float16\n",
        "                                )\n",
        "\n",
        "model, tokenizer = load_model(model_name, bnb_config)\n",
        "for count in range(3):\n",
        "  eos_token_llama = 'assistant'#'[/INST]'\n",
        "  results_column = f\"preds_llama3_{count}\"\n",
        "  predictions = classify(texts, labels, \"VESPA\", eos_token_llama)\n",
        "  add_results_to_csv(vespa_dataset, results_column, predictions)\n",
        "  evaluate(vespa_dataset, all_labels_VESPA, results_column, save_figure=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsTLwkQxT4xB"
      },
      "source": [
        "### Error analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj4qU46LRIPy",
        "outputId": "5bec35a3-24f9-4ec4-ffae-b0add0064f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As a forensic linguistics expert, I have analyzed the text provided and based on the clues mentioned in the prompt, I have classified the native language of the author as \"TUR\" (Turkish). Here are some examples from the text that support my judgment:\n",
            "\n",
            "1. Spelling errors: The author makes a spelling error in the word \"MOTHER EARTH\" (instead of \"MOTHER EARTH\"). This is a common feature of non-native English speakers, particularly those from Turkish. In Turkish, the phrase is \"Anne Dünya\" which means \"Mother Earth\".\n",
            "2. Word choice: The author uses the word \"rennaissance\" (instead of \"renaissance\"). This is another common feature of non-native English speakers, particularly those from Turkish. In Turkish, the word for \"renaissance\" is \"içtiri\" which is a loanword from French.\n",
            "3. Syntactic patterns: The author uses a syntactic pattern that is common in Turkish but less common in English. For example, in the sentence \"Going by the designs that are introduced in the market...\", the use of \"going\" instead of \"going\" is a feature of Turkish. In Turkish, the word for \"introduced\" is \"introduk\" which is a loanword from English.\n",
            "4. Grammatical errors: The author makes a grammatical error in the sentence \"So in the wake of so many facts coming to light...\". The use of \"wake\" instead of \"wake\" is a common feature of non-native English speakers, particularly those from Turkish. In Turkish, the word for \"light\" is \"cihar\" which is a loanword from English.\n",
            "\n",
            "Based on these examples, it is clear that the author is a non-native English speaker, and the language patterns in the text suggest a strong influence of Turkish. Therefore, I have classified the author's native language as \"TUR\" (Turkish).\n",
            "As a forensic linguistics expert, I have analyzed the text provided and classified the native language of the author as \"ARA\" (Arabic). Here are the reasons for my judgment:\n",
            "\n",
            "1. Spelling errors: The text contains several spelling errors, which are common in non-native English speakers. For example, the word \"legislations\" is misspelled as \"legislatins\" in the first sentence. Similarly, the word \"advertisement\" is misspelled as \"advertisement\" in the last sentence. These errors suggest that the author is not a native English speaker.\n",
            "2. Word choice: The text uses words that are not commonly used in native English. For example, the word \"faction\" is used in the second sentence, which is not a common word in everyday English. Similarly, the word \"legislatins\" is used in the first sentence, which is not a commonly used word in English. These choices suggest that the author is not a native English speaker.\n",
            "3. Syntactic patterns: The text uses syntactic patterns that are not commonly used in native English. For example, the sentence structure in the first sentence is not commonly used in English. The sentence starts with a prepositional phrase, which is not a common construction in English. Similarly, the second sentence starts with a subordinate clause, which is not a common construction in English. These patterns suggest that the author is not a native English speaker.\n",
            "4. Grammatical errors: The text contains several grammatical errors that are common in non-native English speakers. For example, the word \"product\" in the first sentence is not in the correct form, it should be \"products\". Similarly, the word \"tempts\" in the last sentence is not a commonly used word in English, it should be \"tries\". These errors suggest that the author is not a native English speaker.\n",
            "\n",
            "Based on these observations, I have classified the author's native language as \"ARA\" (Arabic). The text contains several features that are not commonly used in native English, such as spelling errors, word choice, syntactic patterns, and grammatical errors, which suggest that the author is not a native English speaker.\n",
            "As a forensic linguistics expert, I have carefully analyzed the text provided and classified the native language of the author as \"ARA\" (Arabic). Here are some examples from the text that support this judgment:\n",
            "\n",
            "1. Spelling errors: The text contains several spelling errors, which are a common feature of Arabic language. For instance, the word \"pepole\" (instead of \"people\"), \"apartment\" (instead of \"flat\" or \"apartment\"), and \"repairer\" (instead of \"repairer\" or \"repairman\"). These errors are typical of Arabic language and are not found in other languages like Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, or Turkish.\n",
            "2. Word choice: The text frequently uses Arabic loanwords, such as \"pepole,\" \"apartment,\" and \"repairer.\" These loanwords are not found in other languages, and their presence in the text suggests that the author is writing in Arabic.\n",
            "3. Syntactic patterns: The text exhibits syntactic patterns that are typical of Arabic. For example, the use of the verb \"to be\" (كيان) in the present tense, as in \"They do not give enough time to helping their communities.\" (يتعملون لاعمل على منافسات محافظاتهم). This syntactic pattern is not found in other languages.\n",
            "4. Grammatical errors: The text contains grammatical errors that are typical of Arabic. For instance, the lack of a verb agreement in the sentence \"They do not give enough time to helping their communities.\" (يتعملون لاعملعلام م محافظاتهم). This error is not found in other languages.\n",
            "\n",
            "In conclusion, based on the spelling errors, word choice, syntactic patterns, and grammatical errors in the text, I have classified the native language of the author as \"ARA\" (Arabic).\n",
            "As a forensic linguistics expert, I have carefully analyzed the text provided and based on my findings, I am confident in classifying the native language of the author as \"ARA\" (Arabic).\n",
            "\n",
            "Several features of the text suggest that the author is a native speaker of Arabic. Here are some examples:\n",
            "\n",
            "1. Word Choice: The author frequently employs Arabic words and phrases, such as \"kind of idea,\" \"suppose,\" \"invent something,\" \"share the information,\" and \"introduce something.\" These words and phrases are not typically used in English, but are common in Arabic.\n",
            "2. Spelling Errors: Throughout the text, there are several spelling errors that are more common in Arabic than in English. For instance, the author writes \"celular phones\" instead of \"cellular phones,\" which is a common spelling mistake in Arabic.\n",
            "3. Grammatical Errors: The author makes several grammatical errors that are also common in Arabic. For example, the author writes \"If the engineers and students just learn the facts\" instead of \"If the engineers and students just learn the facts.\" This is a common grammatical error in Arabic, where the verb \"learn\" is used in the present tense instead of the past tense.\n",
            "4. Syntax Patterns: The author's sentence structure is also more common in Arabic. For instance, the author writes \"In conclusion I want to restate that it is more important for students to understand the ideas and concepts than it is for them to learn facts.\" This sentence structure is more common in Arabic than in English, where the subject-verb-conclusion structure is more common.\n",
            "5. Vocabulary: The author uses vocabulary that is more common in Arabic than in English. For example, the author writes \"15 years ago\" instead of \"15 years ago.\" This is a common vocabulary mistake in Arabic, where the number of the verb \"to write\" is used instead of the past tense.\n",
            "\n",
            "Based on these features, I am confident in classifying the native language of the author as \"ARA\" (Arabic).\n",
            "Based on the text provided, I believe that the author's native language is \"CHI\" (Chinese). Here are some examples from the text that support this classification:\n",
            "\n",
            "1. Word choice: The author uses several words that are not commonly found in English, such as \"in twenty years time\" (line 2), \"large roads\" (line 4), \"traffic rules\" (line 4), \" regard\" (line 6), \"alternative\" (line 110), and \"finished\" (line 14). These words suggest that the author is not a native English speaker and may be using English as a second language.\n",
            "\n",
            "2. Spelling errors: The author makes several spelling errors throughout the text, such as \"in twenty years\" instead of \"in twenty years\" (line 2), \"comparing\" instead of \"comparing their advantages and disadvantages\" (line 8), and \"finished\" instead of \"finished\" (line 14). These errors are common in non-native English speakers' writing.\n",
            "\n",
            "3. Syntactic patterns: The author uses some sentence structures that are not typical of English. For example, in line 8, the author writes \"For instance, we will still be using our cars to go near distances.\" This sentence structure is not typical of English, where the subject-verb-object word order is typically followed.\n",
            "\n",
            "4. Grammatical errors: The author makes several grammatical errors throughout the text. For example, in line 10, the author writes \"So it's logical to think that people are already fed up with problems that petrol cause.\" Here, the author uses the word \"fed\" instead of \"fed up\", which is a common English idiom.\n",
            "\n",
            "Based on these examples, it is clear that the author is not a native English speaker and may be using English as a second language. Therefore, I classify the author's native language as \"CHI\" (Chinese).\n",
            "As a forensic linguistics expert, I have classified the native language of the author of the given text as \"CHI\" (Chinese). Here's my reasoning and examples from the text that support my judgment:\n",
            "\n",
            "1. Spelling errors: The author makes a spelling error in the word \"intellenge\" (instead of \"intelligence\"). This is a common feature of non-native English speakers, particularly those from East Asian languages like Chinese.\n",
            "2. Word choice: The author chooses to use the word \"tenecity\" (instead of \"tenacity\"). This is another common feature of non-native English speakers, particularly those from East Asian languages like Chinese.\n",
            "3. Syntactic patterns: The author's use of the sentence structure is unusual for a native English speaker. For example, the use of the phrase \"For example\" instead of \"For example, one of the great geniuses of our world is Einstein\" is a feature of Chinese language.\n",
            "4. Grammatical errors: The author makes a grammatical error in the phrase \"does his/her best to achieve his goal\" (instead of \"does his/her best to achieve his goal\"). This is a common feature of non-native English speakers, particularly those from East Asian languages like Chinese.\n",
            "\n",
            "Based on these clues, I have classified the native language of the author of the given text as \"CHI\" (Chinese).\n",
            "\n",
            "It is important to note that while these clues can be helpful in identifying the native language of the author, they are not foolproof, and other factors such as the author's education level, language proficiency, and exposure to English may also play a role in their writing style.\n",
            "As a forensic linguistics expert, I have carefully analyzed the text provided and classified the native language of the author as \"CHI\" (Chinese). Here are the reasons behind my judgment:\n",
            "\n",
            "1. Word Choice: The author consistently uses words that are more commonly found in Chinese than in other languages. For instance, the author uses \"on one\" instead of \"one and only\" (a more common phrase in English), and \"grant importance\" instead of \"give importance\" (a more common phrase in Chinese).\n",
            "2. Syntactic Patterns: The author's sentence structures are more similar to those found in Chinese than in other languages. For example, the author uses a subject-verb-object word order in some sentences, which is more common in Chinese than in English.\n",
            "3. Grammatical Errors: The author makes several grammatical errors that are more common in Chinese than in other languages. For example, the author writes \"nuke\" instead of \"cook\" (a common mistake in Chinese), and \"impoosible\" instead of \"impossible\" (a common mistake in Chinese).\n",
            "4. Cultural References: The author makes several cultural references that are more common in Chinese than in other languages. For example, the author mentions \"Micheal Jackson\" and \"Pepsi\", which are more commonly known and used in Chinese culture than in other cultures.\n",
            "\n",
            "Examples from the text:\n",
            "\n",
            "* \"On one can change the reality.\" (Word choice, Chinese)\n",
            "* \"Every research about advertisements or advertised products indicated that advertisements has power to manipulate the people's thoughts regarding products.\" (Syntactic patterns Chinese)\n",
            "* \"So  the company use and benefits from this situation.\" (Grammatical errors Chinese)\n",
            "* \"You cannot nuke something it near the baby or radiation sensitive people.\" (Grammatical errors Chinese)\n",
            "* \"In Sum, Advertisements is very powerful tools for affecting people' thoughts about specific product.\" (Cultural references Chinese)\n",
            "\n",
            "Based on these examples, I have determined that the native language of the author of this text is Chinese.\n",
            "As a forensic linguistics expert, I have analyzed the text provided and based on the clues such as spelling errors, word choice, syntactic patterns, and grammatical errors, I have classified the native language of the author as \"GER\" (German).\n",
            "\n",
            "Here are some examples that support my judgment:\n",
            "\n",
            "1. Word Choice: The author uses words such as \"Filling\" (instead of \"Fillling\") and \"Spesification\" (instead of \"Specification\"). These are common errors in German, indicating that the author is a German speaker.\n",
            "2. Syntactic Patterns: The text has a complex sentence structure, typical of German language. For instance, the sentence \"As a man can read a lot about a subject but this will be his habbits about an topic\" is a complex sentence that is common in German language.\n",
            "3. Grammatical Errors: The text has several grammatical errors, such as the lack of articles and incorrect use of verb tenses. For example, the author writes \"But as stated in the first passage\" instead of \"But as stated in the first passage\". These errors are common in German language.\n",
            "4. Consistency: The text consistently uses German grammar and syntax throughout, indicating that the author is a native German speaker.\n",
            "\n",
            "Based on these clues, I am confident that the native language of the author of this text is \"GER\" (German).\n",
            "Based on the text provided, I would classify the author's native language as \"HIN\" (Hindi). \n",
            "\n",
            "There are several reasons for this judgment:\n",
            "\n",
            "1. Word choice: The author frequently uses words that are uncommon or unfamiliar to English speakers, such as \"unpreditable\" (unpredictable), \"marshland\" (marshy land), and \"increse\" (increase). These words suggest that the author is writing in a language that is different from English.\n",
            "\n",
            "2. Spelling errors: The text contains several spelling errors, such as \"trekking\" (trek), \"practically\" (practically), and \"consequantle\" (consequential). These errors are common in non-native English speakers, particularly those who are not fluent in the language.\n",
            "\n",
            "3. Syntactic patterns: The author's sentence structure and syntax are different from those of native English speakers. For example, in the sentence \"For instance consider you want to go to a place which have hered alot about its breathtaking and spectacular view,\" the use of \"have\" instead of \"had\" and the placement of the prepositional phrase \"about its\" are indicative of a language other than English.\n",
            "\n",
            "4. Grammatical errors: The text contains several grammatical errors, such as the lack of articles (e.g., \"there are many unpreditable conditions\" instead of \"there are many unpredictable conditions\") and the use of the wrong verb form (e.g., \"you could visit there by a group\" instead of \"you could visit there in a group\"). These errors suggest that the author is not a native English speaker.\n",
            "\n",
            "\n",
            "Examples from the text that support the judgment of \"HIN\" (Hindi) as the author's native language include:\n",
            "\n",
            "* \"unpreditable\" (unpredictable)\n",
            "* \"marshland\" (marshy land)\n",
            "* \"increse\" (increase)\n",
            "* \"trekking\" (tre)\n",
            "* \"practically\" (practically)\n",
            "* \"consequantle\" (consequential)\n",
            "* \"For instance consider\" (instead of \"For instance consider\")\n",
            "* \"have\" (instead of \"had\")\n",
            "* \"breathtaking\" (instead of \"breathtaking\")\n",
            "* \"spectacular\" (instead of \"spectacular\")\n",
            "* \"there are many unpreditable conditions\" (instead of \"there are many unpredictable conditions\")\n",
            "* \"you could visit there by a group\" (instead of \"you could visit there in a group\")\n",
            "\n",
            "In conclusion, based on the language patterns and errors in the text, it is more likely that the author is writing in \"HIN\" (Hindi) than in any of the other languages listed in the prompt.\n",
            "As a forensic linguistics expert, I have carefully analyzed the text provided and based on the clues offered, I am confident in classifying the native language of the author as \"HIN\" (Hindi). Here are the reasons for my judgment:\n",
            "\n",
            "1. Spelling errors: The author makes a spelling error in the word \"enjoy\" which is a common error in Hindi. In Hindi, the word for \"enjoy\" is \"ENJOY\" (एनजॉय). This suggests that the author is not a native English speaker and is more likely to make such errors.\n",
            "2. Word choice: The author uses the word \"satisfaction\" which is a French loanword in Hindi. In Hindi, the word for \"satisfaction\" is सatisfaction (सएक्षा). This is a common feature of Hindi language, as it has borrowed many words from other languages like French, English, and others.\n",
            "3. Syntactic patterns: The author's use of complex sentence structures and long sentences is typical of Hindi language. For example, in the sentence \"well we all know that we can not grow money on trees so we start working part or full time at an early age\", the use of multiple clauses and the repetition of the word \"full\" is a common feature of Hindi language.\n",
            "4. Grammatical errors: The author makes a grammatical error in the sentence \"on top of working we have to study in order to graduate\". In Hindi, the correct sentence would be \"उपर्युक्त कर्रें संख्य संख्य करें\" (On top of working, we have to study in order to graduate). This error suggests that the author is not a native English speaker and is more likely to make such errors.\n",
            "5. Cultural references: The author's reference to \"retirement\" is a common feature of Hindi language, as in India, retirement is referred to as \"�����ें��ें\" (Oten).\n",
            "\n",
            "In conclusion, based on the analysis of the text, I am confident in classifying the native language of the author as \"HIN\" (Hindi). The spelling errors, word choice, syntactic patterns, grammatical errors, and cultural references all point to the author being a Hindi speaker.\n",
            "Based on the provided text, I strongly classify the native language of the author as \"ITA\" (Italian).\n",
            "\n",
            "There are several clues throughout the text that suggest the author's native language is Italian:\n",
            "\n",
            "1. Word choice: The text frequently uses Italian-specific words and phrases, such as \"leaning process,\" \"conceps,\" \"ideas,\" \"results,\" \"anaylse,\" and \"comparing.\" These words are more commonly used in Italian than in other languages.\n",
            "2. Spelling errors: The text contains no spelling mistakes, which is unusual for a non-native author. Non-native authors often make spelling mistakes in their second language, such as \"colloquial\" instead of \"colloquial.\"\n",
            "3. Syntactic patterns: The text's sentence structure and syntax are more common in Italian than in other languages. For example, the sentence \"To me this is a must for the learning process\" is more common in Italian than in other languages.\n",
            "4. Grammatical errors: The text contains no grammatical errors, which is unusual for a non-native author. Non-native authors often make grammatical mistakes in their second language, such as using the wrong verb tense or subject-verb agreement.\n",
            "\n",
            "Based on these clues, I strongly believe that the author's native language is Italian. Italian.\n",
            "As a forensic linguistics expert, I have analyzed the text provided and based on my findings, I can confidently classify the native language of the author as \"ITA\" (Italian). Here's my reasoning:\n",
            "\n",
            "1. Word Choice: The author consistently uses Italian words and phrases, such as \"travelling\" (instead of \"traveling\"), \"travel\" (instead of \"journey\"), \"tour guide\" (instead of \"tour leader\"), \"hotel\" (instead of \"hotel\"), \"restaurants\" (instead of \"restaurants\"). This suggests that the author is more likely to be a native Italian speaker.\n",
            "2. Spelling errors: The author makes occasional spelling errors, such as \"procedures\" instead of \"procedures\", \"hot\" instead of \"host\", \"culture\" instead of \" cultue\". These errors are common in Italian language, and less common in other languages.\n",
            "3. Syntactic patterns: The author uses sentence structures that are common in Italian language, such as the use of the subjunctive mood in \"if we would like to go to a new place or city\", \"the fact that we might choose a country or a city that the local language is not our mother tongue, so the tour guide would be a helpful resource in this case\".\n",
            "4. Grammatical errors: The author makes grammatical errors such as \"the local language is not our mother tongue\", \"the useful information about the habits and cultures of the people in that specific country\". These errors are common in Italian language.\n",
            "\n",
            "Based on these findings, I can confidently classify the native language of the author as \"ITA\" (Italian). The author's use of Italian words, phrases, spelling errors, syntactic patterns, and grammatical errors all suggest that the author is a native Italian speaker.\n",
            "As a forensic linguistics expert, I have carefully analyzed the text provided and classified the native language of the author as \"JPN\" (Japanese). Here's my reasoning and examples from the text that support my judgment:\n",
            "\n",
            "1. Word choice and vocabulary: The author uses words and phrases that are characteristic of Japanese language, such as \"opportunities\" (JPN: \"chance\" or \"opportunity\"), \"crowdedness\" (JPN: \"crowding\" or \"crowdedness\"), \"traffic jam\" (JPN: \"traffic congestion\" or \"traffic jam\"), and \"civilization\" (JPN: \"civilization\" or \"civilized society\").\n",
            "2. Grammatical errors: The author makes some grammatical errors that are common in Japanese, such as using the present tense instead of the past tense (e.g., \"it is debated\" instead of \"it was debated\").\n",
            "3. Syntactic patterns: The author's sentence structure and sentence structure is similar to that of Japanese, with a tendency to use long, complex sentences with multiple clauses. For example, in the sentence \"A second point is that cars lead to problems...\", the use of a relative clause (e.g., \"A second point\") is more common in Japanese than in English.\n",
            "4. Phonological features: The author's use of English phonology is also consistent with Japanese, such as the tendency to pronounce the letter \"r\" as a flap \"r\" (e.g., \"work\" instead of \"workwork\").\n",
            "\n",
            "Examples from the text that support these observations include:\n",
            "\n",
            "* \"In fact, when the opportunuties, such as comfort and rapid access to locations, it provides is considered, it is the inevitable vehicle of people.\" (JPN: \"When the opportunities, such as comfort and rapid access to locations, it provides is considered, it is the inevitable vehicle of people.\")\n",
            "* \"Although some people may argue argue that cars cause certain problems such as crowdedness or air pollution, I do not foresee any change in the number of cars in use in the coming twenty years.\" (JPN: \"Although some people may argue that cars cause certain problems such as crowdedness or air pollution, I do not foresee any change in the number of cars in use in the coming twenty years.\")\n",
            "* \"A second point is that cars lead to problems such as air pollution and greenhouse effect, but this does not mean that number number of cars in use should decrease.\" (JPN: \"A second point is that cars lead to problems such as air pollution and greenhouse effect, but this does not mean that number number of cars in use should decrease.\")\n",
            "\n",
            "Based on these observations, I have classified the author's native language as Japanese.\n",
            "As a forensic linguistics expert, I have analyzed the text provided and based on the clues such as spelling errors, word choice, syntactic patterns, and grammatical errors, I can confidently classify the native language of the author as \"JPN\" (Japanese).\n",
            "\n",
            "Here are the reasons why I reached this conclusion:\n",
            "\n",
            "1. Spelling errors: The author frequently misspells words, such as \"traveling\" as \"travelling\" and \"sightseeing\" as \"sightseeing\". This is a common feature of Japanese English, where the influence of the native language can result in unique spelling patterns.\n",
            "2. Word choice: The author uses words that are commonly found in Japanese English, such as \"group\" as \"grop\" and \"responsible\" as \"resposible\". This suggests that the author's native language is Japanese, where these words may be more commonly used.\n",
            "3. Syntactic patterns: The author's use of sentence structures is also consistent with Japanese English. For example, the author uses the phrase \"I felt\" instead of \"I felt\" which is a more common pattern in Japanese English.\n",
            "4. Grammatical errors: The author makes grammatical errors, such as using the present tense instead of the past tense, which is a common feature of Japanese English.\n",
            "\n",
            "Examples from the text that support this classification include:\n",
            "\n",
            "* \"I felt so frustrated that I decided then that I will go there alone sometime afterwards.\" (Japanese English often uses the present tense instead of the past tense, as in this example)\n",
            "* \"Personally, therefore, I prefer to travel alone if I have enough time.\" (The use of \"therefore\" instead of \"therefore\" is a common feature of Japanese English)\n",
            "\n",
            "In conclusion, based on the clues provided in the text, I am confident that the author's native language is Japanese.\n",
            "As a forensic linguistics expert, I have carefully analyzed the text provided and classified the native language of the author as \"KOR\" (Korean). Here are the reasons behind my judgment:\n",
            "\n",
            "1. Spelling errors: The text contains a few spelling errors, which can be a useful indicator of the author's native language. For instance, the word \"academic\" is misspelled as \"academic\" in the text. This is a common spelling error in Korean, but less common in other languages. (KOR: 응ademiced, ENG: academic)\n",
            "2. Word choice: Korean speakers tend to use more complex vocabulary and sentence structures than speakers of other languages. For example, the phrase \"to have broad knowledge\" is more commonly expressed in Korean as \"많은 �����ossen 읽는\" (MAN-eun bun-choon-seu-neun), which is more complex than the English equivalent. (KOR: 많은 �����ossen 읽는, ENG: to have broad knowledge)\n",
            "3. Syntactic patterns: Korean language has a distinct syntactic structure that can be identified in written texts. For instance, the use of the particle \"는\" (neun) to indicate possession is more common in Korean than in other languages. In the text, the phrase \"because you work on only one topic\" contains this particle, which is a characteristic of Korean syntax. (KOR: 당신 논항목 작성, ENG: because you work on only one topic)\n",
            "4. Grammatical errors: The text contains a few grammatical errors that suggest a Korean authorship. For instance, the phrase \"you can do your researches broadly\" contains an incorrect use of the verb \"do,\" which is more commonly spelled \"study\" in Korean. (KOR: 당신 분 읽는, ENG: you can study broadly)\n",
            "\n",
            "Based on these linguistic features, I have classified the author's native language as Korean (KOR). While the text may contain some errors or variations in spelling, vocabulary, and syntax, these are consistent with the linguistic patterns of the Korean language.\n",
            "As a forensic linguistics expert, I have analyzed the text provided and based on the clues such as spelling errors, word choice, syntactic patterns, and grammatical errors, I have classified the native language of the author as \"TEL\" (Telugu).\n",
            "\n",
            "Here are the reasons for my judgment:\n",
            "\n",
            "1. Spelling errors: The author makes a consistent spelling error in the word \"science\" throughout the text. In Telugu, the word for \"science\" is \"scince\" (�����్������ ����� క క క క క). This spelling error suggests that the author is not a native English speaker and is more likely to be a Telugu speaker.\n",
            "2. Word choice: The author uses the word \"academic\" instead of \"academic\" in the sentence \"Having no knowledge about other academic subjects is something that will always limit us during our studies.\" This suggests that the author is not a native English speaker and is more likely to be a Telugu speaker.\n",
            "3. Syntactic patterns: The author uses a complex sentence structure in the text, which is not uncommon in Telugu. For example, in the sentence \"If we really want to be good in our jobs, this can not be succeeded by just concentrating on one subject.,\" the author uses a complex sentence structure with multiple clauses, which is more common in Telugu.\n",
            "4. Grammatical errors: The author makes a grammatical error in the sentence \"In conclusion, I agree with the statement that it is better to have broad knowledge of many academic subjects.\" The author should have written \"In conclusion, I agree with the statement that it is better to have broad knowledge of many academic subjects.\" This grammatical error suggests that the author is not a native English speaker and is more likely to be a Telugu speaker.\n",
            "\n",
            "In conclusion, based on the above analysis, I have classified the native language of the author as \"TEL\" (Telugu).\n"
          ]
        }
      ],
      "source": [
        "commonly_confusedpairs = [(\"JPN\", \"KOR\"),\n",
        "                          (\"SPA\", \"ITA\"),\n",
        "                          (\"ITA\", \"FRE\"),\n",
        "                          (\"SPA\", \"FRE\"),\n",
        "                          (\"HIN\", \"TEL\"),\n",
        "                          ('ARA', 'TUR'),\n",
        "                          (\"KOR\", \"CHI\")]\n",
        "results_TOEFL = \"/content/drive/MyDrive/thesis_NLI/TOEFL11/toefl_results.csv\"\n",
        "new_fp_reasonings = \"/content/drive/MyDrive/thesis_NLI/TOEFL11/erroranalysis_llama-7b-zero-closed.csv\"\n",
        "df = pd.read_csv(results_TOEFL)\n",
        "reasonings = error_analysis_LLM(df, \"TOEFL\", \"preds_zero_llama7b\", commonly_confusedpairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIB8Sx1iRWoT"
      },
      "source": [
        "## Gemma-7b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/gemma-7b-it\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_compute_dtype=torch.float16\n",
        "                                )\n",
        "\n",
        "model, tokenizer = load_model(model_name, bnb_config)\n",
        "for count in range(3):\n",
        "  eos_token = 'model'\n",
        "  results_column = f\"preds_gemma_{count}\"\n",
        "  predictions = classify(texts, labels, \"VESPA\", eos_token)\n",
        "  add_results_to_csv(vespa_dataset, results_column, predictions)\n",
        "  evaluate(vespa_dataset, all_labels_VESPA, results_column, save_figure=True)"
      ],
      "metadata": {
        "id": "B2bk6x6xQpHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW1YljDvRgXx"
      },
      "source": [
        "## Mistral-7b-instruct for NLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl5B59EURlQw"
      },
      "source": [
        "### Loading model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwcEQIwBRq5L"
      },
      "outputs": [],
      "source": [
        "# Load model with model name and bitsandbytes configuration\n",
        "for count in range(1,3):\n",
        "  model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "  bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_compute_dtype=torch.float16\n",
        "                                )\n",
        "\n",
        "  model, tokenizer = load_model(model_name, bnb_config)\n",
        "  eos_token = '[/INST]'\n",
        "  results_column = f\"preds_zero_mistral_7b_open_{count}\"\n",
        "  predictions = classify(texts, labels, \"ICLE\", eos_token, results_ICLE, results_column, closedopen_setting='open')\n",
        "  add_results_to_csv(results_ICLE, results_column, predictions)\n",
        "  evaluate(results_ICLE, all_labels_ICLE_open, results_column, save_figure=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvRMT4_rgviw"
      },
      "source": [
        "## Phi-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToLEWg6BhZiY"
      },
      "source": [
        "### Loading model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7aSaLddgrnr"
      },
      "outputs": [],
      "source": [
        "for count in range(1,3):\n",
        "  # model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "  # bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "  #                               bnb_4bit_compute_dtype=torch.float16)\n",
        "\n",
        "  # model, tokenizer = load_model(model_name, bnb_config)\n",
        "  eos_token='|'\n",
        "  results_ICLE = \"/content/drive/MyDrive/thesis_NLI/ICLE-NLI-results.csv\"\n",
        "  results_TOEFL = \"/content/drive/MyDrive/thesis_NLI/TOEFL11/toefl_results.csv\"\n",
        "  results_column = f\"preds_zero_phi3_open_{count}\"\n",
        "  predictions = classify(texts, labels, \"ICLE\", eos_token, results_ICLE, results_column, closedopen_setting='open')\n",
        "  add_results_to_csv(results_ICLE, results_column, predictions)\n",
        "  evaluate(results_ICLE, all_labels_ICLE_open, results_column, save_figure=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpTo1Tr1L912"
      },
      "source": [
        "# Error analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE2-qkY6MBXC"
      },
      "source": [
        "## Defining prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52FRtz4GMEej"
      },
      "outputs": [],
      "source": [
        "sysprompt_erroranalysis_TOEFL = '''\n",
        "  You are a forensic linguistics expert that reads English texts written by non-native authors to classify the native language of the author as one of:\n",
        "  \"ARA\": Arabic\n",
        "  \"CHI\": Chinese\n",
        "  \"FRE\": French\n",
        "  \"GER\": German\n",
        "  \"HIN\": Hindi\n",
        "  \"ITA\": Italian\n",
        "  \"JPN\": Japanese\n",
        "  \"KOR\": Korean\n",
        "  \"SPA\": Spanish\n",
        "  \"TEL\": Telugu\n",
        "  \"TUR\": Turkish\n",
        "  Use clues such as spelling errors, word choice, syntactic patterns, and grammatical errors to decide on the native language of the author.\\n\n",
        "\n",
        "  DO NOT USE ANY OTHER CLASS.\n",
        "  IMPORTANT: Do not classify any input as \"ENG\" (English). English is an invalid choice.'''\n",
        "\n",
        "sysprompt_erroranalysis_ICLE = '''\n",
        "  You are a forensic linguistics expert that reads English texts written by non-native authors to classify the native language of the author as one of:\n",
        "  \"BUL\": Bulgarian\n",
        "  \"CHI\": Chinese\n",
        "  \"CZE\": Czech\n",
        "  \"FRE\": French\n",
        "  \"JPN\": Japanese\n",
        "  \"RUS\": Russian\n",
        "  \"SPA\": Spanish\n",
        "  Use clues such as spelling errors, word choice, syntactic patterns, and grammatical errors to decide on the native language of the author.\\n\\n\n",
        "\n",
        "  DO NOT USE ANY OTHER CLASS.\n",
        "  IMPORTANT: Do not classify any input as \"ENG\" (English). English is an invalid choice.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQSp-ZaRWDwH"
      },
      "outputs": [],
      "source": [
        "def get_sample(results_fp, preds_column, list_classpairs):\n",
        "  '''\n",
        "  Create small sample that contains some correctly and wrongly classified samples based on the provided list of pairs of classes\n",
        "  :param results_fp: filepath to results\n",
        "  :param preds_column: column that contains previous predictions\n",
        "  :param list_classpairs: list with pairs of classes\n",
        "  :type results_fp: str\n",
        "  :type preds_column: str\n",
        "  :type list_classpairs: list with dictionaries\n",
        "  '''\n",
        "  if \"toefl\" in results_fp: # determine which dataset it is, because the prompts and classes are different for each\n",
        "    sys_prompt = sysprompt_erroranalysis_TOEFL\n",
        "    all_labels = all_labels_TOEFL\n",
        "    dataset='TOEFL'\n",
        "  elif \"ICLE\" in results_fp:\n",
        "    sys_prompt = sysprompt_erroranalysis_ICLE\n",
        "    all_labels = all_labels_ICLE\n",
        "    dataset='ICLE'\n",
        "\n",
        "  df = pd.read_csv(results_fp)\n",
        "  rows_list = []\n",
        "  df = df.sample(frac=1)\n",
        "  df['goldpred'] = df['language'].astype(str) + df[preds_column]\n",
        "  filenames = df['filename'].tolist()\n",
        "  texts = df['text'].tolist()\n",
        "  labels = df['language'].tolist()\n",
        "  preds = df[preds_column].tolist()\n",
        "  goldpreds = df['goldpred'].tolist()\n",
        "\n",
        "  selected_files = ['1049746', '1017679', '1072659', '1097242', '1108141', '1160675'] # files that were used by Zhang & Salle\n",
        "  correct_predictions=0\n",
        "  for filename, text, label, pred, goldpred in zip(filenames, texts, labels, preds, goldpreds):\n",
        "    row_dict = {'filename': filename,\n",
        "                'text':text,\n",
        "                'language':label,\n",
        "                'prediction': pred,\n",
        "                'goldpreds': goldpred\n",
        "                }\n",
        "    if label!=pred:\n",
        "      for label1, label2 in list_classpairs:\n",
        "        if label == 'TUR':\n",
        "          continue\n",
        "        elif str(label)!=label1 and str(pred)!=label2 or str(label)!=label2 and str(pred)!= label1:\n",
        "          rows_list.append(row_dict)\n",
        "      # continue\n",
        "  headers = ['filename', 'text', 'language', 'prediction', 'goldpreds']\n",
        "  sample_df1 = pd.DataFrame(rows_list, columns=headers)\n",
        "  sample_df1=sample_df1[:20]\n",
        "  print(len(sample_df1))\n",
        "\n",
        "  rows_list = []\n",
        "  for filename, text, label, pred, goldpred in zip(filenames, texts, labels, preds, goldpreds):\n",
        "    row_dict = {'filename': filename,\n",
        "                'text':text,\n",
        "                'language':label,\n",
        "                'prediction': pred,\n",
        "                'goldpreds': goldpred\n",
        "                }\n",
        "    for f in selected_files:\n",
        "      if f in filename and dataset=='TOEFL':\n",
        "        rows_list.append(row_dict)\n",
        "    if label == pred:\n",
        "      if correct_predictions >=15:\n",
        "        continue\n",
        "      else:\n",
        "        rows_list.append(row_dict)\n",
        "        correct_predictions+=1\n",
        "  sample_df2 = pd.DataFrame(rows_list, columns=headers)\n",
        "  sample_df = pd.concat([sample_df1, sample_df2], ignore_index=True)\n",
        "  sample_df = sample_df.drop_duplicates()\n",
        "  print(len(sample_df2))\n",
        "\n",
        "  new_csv = f'/content/drive/MyDrive/thesis_NLI/{dataset}_results/explainability_sample.csv'\n",
        "  sample_df = sample_df.drop('goldpreds', axis=1)\n",
        "  print(len(sample_df))\n",
        "  sample_df.to_csv(new_csv)\n",
        "  sample_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_ICLE = \"/content/drive/MyDrive/thesis_NLI/ICLE-NLI-results-final.csv\"\n",
        "list_classpairs = [(\"BUL\", \"RUS\"),\n",
        "                   (\"CZE\", \"RUS\"),\n",
        "                   (\"SPA\", \"FRE\"),\n",
        "                   (\"CZE\", \"BUL\"),\n",
        "                   (\"SPA\", \"BUL\"),\n",
        "                   (\"RUS\", \"FRE\"),\n",
        "                   (\"JPN\", \"CHI\"),\n",
        "                   (\"CZE\", \"SPA\")]\n",
        "get_sample(results_ICLE, 'preds_gpt4', list_classpairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGVh62MgPB1L",
        "outputId": "ac533116-88f0-4d9e-d94c-e81534cc6f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n",
            "40\n",
            "59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_reasonings(sample_fp):\n",
        "  if 'TOEFL' in sample_fp:\n",
        "    sys_prompt = sysprompt_erroranalysis_TOEFL\n",
        "  elif 'ICLE' in sample_fp:\n",
        "    sys_prompt = sysprompt_erroranalysis_ICLE\n",
        "  sample_df = pd.read_csv(sample_fp)\n",
        "  reasonings = []\n",
        "  count=1\n",
        "  for text, pred in zip(sample_df['text'], sample_df['prediction']):\n",
        "    main_task = f'''{text}You must provide a guess. Output two named sections: (1) \"Native Language\" with the name of the language, and (2) \"Reasoning\" with a detailed explanation of your judgement with examples from the text.'''\n",
        "    messages = [\n",
        "        {'role': \"system\", \"content\": sys_prompt},\n",
        "        {'role': \"user\", \"content\": main_task}]\n",
        "    # messages = [{'role': \"user\", \"content\": sys_prompt + \"\\n\" +main_task }]\n",
        "    fullprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    # fullprompt = \"Instruction: \" + sys_prompt + '\\n\\n' + main_task + '\\n' + '\\n\\nInput: '+ text + \"\\nResponse:\"\n",
        "\n",
        "    output = generate_text(fullprompt) # generate text per TOEFL text\n",
        "    output_only = clean_output(output, filter_token, output_only=True)\n",
        "    print(count)\n",
        "    print(output_only)\n",
        "    count+=1\n",
        "    reasonings.append(output_only)\n",
        "  column = len(sample_df.columns)\n",
        "  sample_df.insert(column, 'explanations_llama3', reasonings)\n",
        "  sample_df.to_csv(sample_fp, index=False)\n",
        "  return reasonings"
      ],
      "metadata": {
        "id": "9J4L0SS99HLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_compute_dtype=torch.float16\n",
        "                                )\n",
        "\n",
        "model, tokenizer = load_model(model_name, bnb_config)\n",
        "filter_token = 'assistant'\n",
        "results_icle = \"/content/drive/MyDrive/thesis_NLI/ICLE_results/icle_explainability.csv\"\n",
        "results_TOEFL = \"/content/drive/MyDrive/thesis_NLI/TOEFL_results/explainability_sample.csv\"\n",
        "\n",
        "llama_toefl_explanations = add_reasonings(results_TOEFL)"
      ],
      "metadata": {
        "id": "rqrWjestGrcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Postprocessing\n",
        "The following code was used for post-processing, to make sure that all open-set predictions are parsed correctly. For example, predictions like 'FRA' or 'FREN' should be identified as 'FRE', i.e., French."
      ],
      "metadata": {
        "id": "h7Zz8roZLFql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(results_TOEFL)\n",
        "columns = df.columns.tolist()\n",
        "for c in columns:\n",
        "  new_preds = []\n",
        "  unknown_labels = []\n",
        "  all_labels = all_labels_TOEFL_open\n",
        "  # print(c)\n",
        "  if 'open' in c:\n",
        "    preds = df[c].tolist()\n",
        "    for pred in preds:\n",
        "      pred = str(pred)\n",
        "      if pred not in all_labels: # if prediction is not in the predefined set of classes\n",
        "        if pred == 'es':\n",
        "          pred = 'SPA'\n",
        "        elif pred == 'en' or pred == 'EN' or pred == 'en-GB' or pred == 'eng' or pred == 'Enlish':\n",
        "          pred = 'ENG'\n",
        "        elif pred == 'Unknown' or pred == 'nan' or pred == 'familiar' or pred == 'local' or pred == 'nowadays' or pred == 'advertisements':\n",
        "          pred = 'other'\n",
        "        elif '0' in str(pred):\n",
        "          pred = 'ENG'\n",
        "        elif pred == 'pt':\n",
        "          pred = 'Portuguese'\n",
        "        elif pred == 'fr' or pred == 'Français':\n",
        "          pred = 'FRE'\n",
        "        elif pred == 'ar':\n",
        "          pred = 'ARA'\n",
        "        elif len(pred)>30:\n",
        "          pred = 'other'\n",
        "        elif '{' in pred:\n",
        "          pred = 'other'\n",
        "        elif pred=='zh-CN' or pred == 'zh-Hans' or pred == '中文':\n",
        "          pred='CHI'\n",
        "        elif pred == 'it':\n",
        "          pred='ITA'\n",
        "        elif pred == 'de':\n",
        "          pred='GER'\n",
        "        elif pred == 'pl' or pred == ' Polish' or pred == 'Polska':\n",
        "          pred = 'Polish'\n",
        "        elif pred == 'cs':\n",
        "          pred = 'CZE'\n",
        "        elif pred == 'ja':\n",
        "          pred = 'JPN'\n",
        "        elif pred =='ru' or pred == 'rus':\n",
        "          pred = 'RUS'\n",
        "        elif pred == 'nl':\n",
        "          pred = 'Dutch'\n",
        "        elif len(pred)<=1 or pred == 'und' or pred == 'non-':\n",
        "          pred = 'other'\n",
        "        else:\n",
        "          unknown_labels.append(pred)\n",
        "      new_preds.append(pred)\n",
        "    print(c)\n",
        "    df = df.drop(c, axis=1)\n",
        "    num_columns = len(df.columns)\n",
        "    df.insert(num_columns, c, new_preds)\n",
        "    for label in set(unknown_labels):\n",
        "      print(label)\n",
        "    print(len(new_preds))\n",
        "    print('\\n\\n')\n",
        "df.to_csv(results_TOEFL)\n",
        "    # print(set(unknown_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXArY_B7HWAH",
        "outputId": "de02b185-5af8-4082-a5b1-975f2c03d8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds_zero_llama3_8b_openclass\n",
            "Persian (Farsi)\n",
            "RUS\n",
            "Indonesian\n",
            "Portuguese\n",
            "Vietnamese\n",
            "Persian/Farsi\n",
            "Indian\n",
            "Malay\n",
            "Farsi/Persian\n",
            "urdu\n",
            "Persian\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_llama2_7b_openclass\n",
            "Brazilian Portuguese\n",
            "RUS\n",
            "Tamil\n",
            "Indonesian\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_gemma_7b_openclass\n",
            "Taiwanese\n",
            "Indonesian\n",
            "Malaysian\n",
            "Egyptian-American\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_mistral_7b_open\n",
            "India\n",
            "Portuguese\n",
            "Persian\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_phi3_open\n",
            "RUS\n",
            "Indonesian\n",
            "Portuguese\n",
            "Taiwanese\n",
            "BUL\n",
            "Malaysian\n",
            "Dutch\n",
            "Malay\n",
            "Bangla\n",
            "Persian\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_llama3_8b_open_1\n",
            "Persian (Farsi)\n",
            "RUS\n",
            "Indonesian\n",
            "Portuguese\n",
            "Vietnamese\n",
            "Thai\n",
            "Persian/Farsi\n",
            "Indian\n",
            "Malay\n",
            "Urdu\n",
            "Persian\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_llama3_8b_open_2\n",
            "Persian (Farsi)\n",
            "RUS\n",
            "Indonesian\n",
            "Portuguese\n",
            "Vietnamese\n",
            "Thai\n",
            "Persian/Farsi\n",
            "Indian\n",
            "Malay\n",
            "Farsi/Persian\n",
            "Persian\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_llama2_7b_open_1\n",
            "Taiwanese Hokkien\n",
            "Brazilian Portuguese\n",
            "RUS\n",
            "Tamil\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_llama2_7b_open_2\n",
            "Brazilian Portuguese\n",
            "RUS\n",
            "Tamil\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_gemma_7b_open_1\n",
            "Taiwanese\n",
            "Indonesian\n",
            "Malaysian\n",
            "Egyptian-American\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_gemma_7b_open_2\n",
            "Taiwanese\n",
            "Indonesian\n",
            "Malaysian\n",
            "Egyptian-American\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_mistral_7b_open_1\n",
            "India\n",
            "Portuguese\n",
            "Persian\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_mistral_7b_open_2\n",
            "India\n",
            "Portuguese\n",
            "Persian\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_phi3_open_1\n",
            "Bangladeshi\n",
            "Kore\n",
            "RUS\n",
            "Bengali\n",
            "Indonesian\n",
            "Arab\n",
            "BUL\n",
            "Malaysian\n",
            "Dutch\n",
            "Turk\n",
            "Polish\n",
            "Bangla\n",
            "Persian\n",
            "1100\n",
            "\n",
            "\n",
            "\n",
            "preds_zero_phi3_open_2\n",
            "Bangladeshi\n",
            "Kore\n",
            "RUS\n",
            "Bengali\n",
            "Indonesian\n",
            "Arab\n",
            "BUL\n",
            "Malaysian\n",
            "Dutch\n",
            "Turk\n",
            "Polish\n",
            "Bangla\n",
            "Persian\n",
            "1100\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "542r2h250cMN",
        "TsTLwkQxT4xB"
      ],
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}